{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM for Text Genaration with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krupc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\gpt2-text-generator-T8z7-MBX-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import argparse\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Used to Load a Pretrained LLM\n",
    "\n",
    "**GPT2LMHeadModel**: This is the model class for GPT-2 (Generative Pretrained Transformer 2), a powerful language model capable of generating human-like text. The 'LMHead' part indicates that this variant of the model includes a language modeling head on top of the basic GPT-2 model, making it suitable for tasks like text generation.\n",
    "\n",
    "**GPT2Tokenizer**: This is a tokenizer specifically designed for the GPT-2 model. A tokenizer is used to convert text into a format that the model can understand and process. For GPT-2, this involves converting text into tokens (such as words or subwords) and encoding these tokens as numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Tokenizer\n",
    "\n",
    "https://huggingface.co/gpt2-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krupc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\gpt2-text-generator-T8z7-MBX-py3.12\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='gpt2-large', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**from_pretrained**: This is a method from the GPT2Tokenizer class. The `from_pretrained` method is used to load a tokenizer that has already been trained on a specific dataset. In this case, it is trained to work with the GPT-2 model.\n",
    "\n",
    "**\"gpt2-large\"**: This is a string argument for the `from_pretrained` method. It specifies which variant of the GPT-2 model tokenizer you want to use. The `gpt2-large` model is a larger version of GPT-2, meaning it has more parameters and can potentially generate more coherent and contextually relevant text compared to smaller versions. The tokenizer for `gpt2-large` is specifically trained to work well with this model variant.\n",
    "\n",
    "**tokenizer**: This is the name of the variable assigned to the initialized tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id = tokenizer.eos_token_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pad_token_id**: This parameter defines the token the model uses for padding. Padding is used to ensure shorter sequences are extended to match the length of the longest sequence when processing a batch of text data.\n",
    "\n",
    "**tokenizer.eos_token_id**: The `eos_token_id` is the ID of the 'end of string' token in the tokenizer. This line sets the padding token ID to be the same as the end-of-string token ID. It tells the model to treat padding in the same way as the end-of-string token, which is important for maintaining consistency in how the model processes text sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text with the LLM}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Text\n",
    "prompt = 'Tell me about the Japan history after the second world war'\n",
    "\n",
    "# Tokeniza o texto do prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**encode**: This is a method of the tokenizer. It takes a text sequence and converts it into a list of numerical tokens. Each token represents a word or part of a word.\n",
    "\n",
    "**return_tensors='pt'**: This argument tells the tokenizer to return the tokens in a format suitable for PyTorch (denoted by 'pt')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24446,   502,   546,   262,  2869,  2106,   706,   262,  1218,   995,\n",
       "          1175]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizes the prompt text\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tensor represents a PyTorch tensor containing a sequence of numerical tokens. These tokens result from processing text through the tokenizer, specifically the GPT-2 tokenizer in this case.\n",
    "\n",
    "The tokenizer has a vocabulary of tokens (words, subwords, symbols, etc.), and each unique text chunk is assigned a specific number. For example, the word *\"hello\"* might be converted into the number 1256, and *\"world\"* could become 5678.\n",
    "\n",
    "The sequence above corresponds to the tokenized form of the input sentence. Each number maps to a word or part of a word in that sentence.\n",
    "\n",
    "The double brackets indicate that this is a two-dimensional tensor. In this context, a single sequence (a sentence) is being represented, so there is only one row in this 2D tensor.\n",
    "\n",
    "The numerical values have no inherent meaning outside the tokenizer's vocabulary context. To understand which text each number represents, you'd need to look up these IDs in the tokenizer's vocabulary.\n",
    "\n",
    "This tensor format is typically fed into a machine learning model like GPT-2 to perform tasks such as text generation, classification, or question answering. The model reads these numbers and uses its trained neural network to interpret them and generate appropriate outputs based on its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating text from prompt with initial text\n",
    "generated_text = model.generate(input_ids, max_length = 100, num_beams = 5, no_repeat_ngram_size = 2, early_stopping = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Generation Parameters:\n",
    "- **max_length=100**: This parameter sets the maximum length of the sequence to be generated. The value of 100 includes both the length of the input text (context) and the new text that the model will generate. The model stops generating additional tokens when this length is reached.\n",
    "\n",
    "- **num_beams=5**: This parameter enables beam search with 5 beams. Beam search is a technique in NLP for text generation where the model considers multiple possible next words at each step and keeps the most promising sequences (or \"beams\") of tokens. Using 5 beams means the model tracks 5 potential sequences at each step, which can lead to higher-quality results but requires more computational resources.\n",
    "\n",
    "- **no_repeat_ngram_size=2**: This setting prevents the model from repeating the same n-grams (in this case, sequences of 2 tokens) in the output text. It helps reduce repetitiveness in the generated text.\n",
    "\n",
    "- **early_stopping=True**: This parameter tells the model to stop generating text as soon as all beam candidates reach the end-of-sentence token. It can make the generation process more efficient by halting the search as soon as a satisfactory output is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24446,   502,   546,   262,  2869,  2106,   706,   262,  1218,   995,\n",
      "          1175,    13,   198,   198,    40,   373,  4642,   290,  4376,   287,\n",
      "          2869,    13,   314,  6348,   510,   287, 11790,    11,   290,   314,\n",
      "          1816,   284,  1524,   287, 42429,    13,  1649,   314,   373,   287,\n",
      "          1029,  1524,    11,   314,  3888,   284, 11790,   284,  2050,   379,\n",
      "           262,  2059,   286, 11790,    13,  1629,   262,   640,    11,   612,\n",
      "           373,   257,  1256,   286, 12097,  1022,  2869,   290,   262,  1578,\n",
      "          1829,   780,   286,   262,  5498,  2159,  1810,    13,   383,  4960,\n",
      "          1230,   373,  2111,   284,   787,  4167,   351,   262,  3399,    11,\n",
      "           475,   262,  4960,   661,   547,   407,  3772,   546,   340,    13]])\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding the Result with the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about the Japan history after the second world war.\n",
      "\n",
      "I was born and raised in Japan. I grew up in Tokyo, and I went to school in Osaka. When I was in high school, I moved to Tokyo to study at the University of Tokyo. At the time, there was a lot of tension between Japan and the United States because of the Second World War. The Japanese government was trying to make peace with the Americans, but the Japanese people were not happy about it.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated_text[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer not only converts text into tokens (numerical representations) but can also do the reverse—converting tokens back into text.\n",
    "\n",
    "**skip_special_tokens=True**: This argument instructs the decoder to ignore special tokens, such as padding tokens or end-of-sequence tokens, which are used for model processing but are not meaningful in the final generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
