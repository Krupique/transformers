{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "import transformers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 2)\n",
      "(2000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>feeling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am feeling completely overwhelmed i have two strategies that help me to feel grounded pour my heart out in my journal in the form of a letter to god and then end with a list of five things i am most grateful for</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have the feeling she was amused and delighted</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i was able to help chai lifeline with your support and encouragement is a great feeling and i am so glad you were able to help me</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                    text  \\\n",
       "0  i am feeling completely overwhelmed i have two strategies that help me to feel grounded pour my heart out in my journal in the form of a letter to god and then end with a list of five things i am most grateful for   \n",
       "1                                                                                                                                                                        i have the feeling she was amused and delighted   \n",
       "2                                                                                      i was able to help chai lifeline with your support and encouragement is a great feeling and i am so glad you were able to help me   \n",
       "\n",
       "  feeling  \n",
       "0    fear  \n",
       "1     joy  \n",
       "2     joy  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>feeling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel like my only role now would be to tear your sails with my pessimism and discontent</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel just bcoz a fight we get mad to each other n u wanna make a publicity n let the world knows about our fight</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i feel like reds and purples are just so rich and kind of perfect</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 text  \\\n",
       "0                           i feel like my only role now would be to tear your sails with my pessimism and discontent   \n",
       "1  i feel just bcoz a fight we get mad to each other n u wanna make a publicity n let the world knows about our fight   \n",
       "2                                                   i feel like reds and purples are just so rich and kind of perfect   \n",
       "\n",
       "   feeling  \n",
       "0  sadness  \n",
       "1    anger  \n",
       "2      joy  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/dados_treino.txt', header=None, delimiter=';')\n",
    "df_test = pd.read_csv('data/dados_teste.txt', header=None, delimiter=';')\n",
    "\n",
    "df_train = df_train.rename(columns= {0: 'text', 1: 'feeling'})\n",
    "df_test = df_test.rename(columns= {0: 'text', 1: 'feeling'})\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "display(df_train.head(3))\n",
    "display(df_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feeling\n",
       "joy         5362\n",
       "sadness     4666\n",
       "anger       2159\n",
       "fear        1937\n",
       "love        1304\n",
       "surprise     572\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['feeling'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feeling\n",
       "joy         695\n",
       "sadness     581\n",
       "anger       275\n",
       "fear        224\n",
       "love        159\n",
       "surprise     66\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['feeling'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The column **text** is going to be the input feature and **feeling** is going to be the output target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing using Spacy\n",
    "\n",
    "[Oficial site](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dict\n",
    "spacy_nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the 'data_preprocessing' function that receives a text as a parameter\n",
    "def data_preprocessing(text):\n",
    "\n",
    "    # Process the text using the dictionary\n",
    "    doc = spacy_nlp(text)\n",
    "\n",
    "    # Creates a list of lemmas from the tokens, converted to lowercase and without whitespace,\n",
    "    # excluding words that are stopwords\n",
    "    tokens = [token.lemma_.lower().strip() for token in doc if not token.is_stop]\n",
    "\n",
    "    # Returns the processed tokens as a single string, joining them with spaces\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['transformed_text'] = df_train['text'].apply(data_preprocessing)\n",
    "df_test['transformed_text'] = df_test['text'].apply(data_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>feeling</th>\n",
       "      <th>transformed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am feeling completely overwhelmed i have two strategies that help me to feel grounded pour my heart out in my journal in the form of a letter to god and then end with a list of five things i am most grateful for</td>\n",
       "      <td>fear</td>\n",
       "      <td>feel completely overwhelmed strategy help feel ground pour heart journal form letter god end list thing grateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have the feeling she was amused and delighted</td>\n",
       "      <td>joy</td>\n",
       "      <td>feeling amuse delight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i was able to help chai lifeline with your support and encouragement is a great feeling and i am so glad you were able to help me</td>\n",
       "      <td>joy</td>\n",
       "      <td>able help chai lifeline support encouragement great feeling glad able help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i already feel like i fucked up though because i dont usually eat at all in the morning</td>\n",
       "      <td>anger</td>\n",
       "      <td>feel like fuck not usually eat morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i still love my so and wish the best for him i can no longer tolerate the effect that bm has on our lives and the fact that is has turned my so into a bitter angry person who is not always particularly kind to the people around him when he is feeling stressed</td>\n",
       "      <td>sadness</td>\n",
       "      <td>love wish good long tolerate effect bm life fact turn bitter angry person particularly kind people feel stress</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                  text  \\\n",
       "0                                                i am feeling completely overwhelmed i have two strategies that help me to feel grounded pour my heart out in my journal in the form of a letter to god and then end with a list of five things i am most grateful for   \n",
       "1                                                                                                                                                                                                                      i have the feeling she was amused and delighted   \n",
       "2                                                                                                                                    i was able to help chai lifeline with your support and encouragement is a great feeling and i am so glad you were able to help me   \n",
       "3                                                                                                                                                                              i already feel like i fucked up though because i dont usually eat at all in the morning   \n",
       "4  i still love my so and wish the best for him i can no longer tolerate the effect that bm has on our lives and the fact that is has turned my so into a bitter angry person who is not always particularly kind to the people around him when he is feeling stressed   \n",
       "\n",
       "   feeling  \\\n",
       "0     fear   \n",
       "1      joy   \n",
       "2      joy   \n",
       "3    anger   \n",
       "4  sadness   \n",
       "\n",
       "                                                                                                   transformed_text  \n",
       "0  feel completely overwhelmed strategy help feel ground pour heart journal form letter god end list thing grateful  \n",
       "1                                                                                             feeling amuse delight  \n",
       "2                                        able help chai lifeline support encouragement great feeling glad able help  \n",
       "3                                                                            feel like fuck not usually eat morning  \n",
       "4    love wish good long tolerate effect bm life fact turn bitter angry person particularly kind people feel stress  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>feeling</th>\n",
       "      <th>transformed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel like my only role now would be to tear your sails with my pessimism and discontent</td>\n",
       "      <td>sadness</td>\n",
       "      <td>feel like role tear sail pessimism discontent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel just bcoz a fight we get mad to each other n u wanna make a publicity n let the world knows about our fight</td>\n",
       "      <td>anger</td>\n",
       "      <td>feel bcoz fight mad n u wanna publicity n let world know fight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i feel like reds and purples are just so rich and kind of perfect</td>\n",
       "      <td>joy</td>\n",
       "      <td>feel like red purple rich kind perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im not sure the feeling of loss will ever go away but it may dull to a sweet feeling of nostalgia at what i shared in this life with my dad and the luck i had to have a dad for years</td>\n",
       "      <td>sadness</td>\n",
       "      <td>m sure feeling loss away dull sweet feeling nostalgia share life dad luck dad year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i feel like ive gotten to know many of you through comments and emails and for that im appreciative and glad you are a part of this little space</td>\n",
       "      <td>joy</td>\n",
       "      <td>feel like ve get know comment email m appreciative glad little space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                     text  \\\n",
       "0                                                                                               i feel like my only role now would be to tear your sails with my pessimism and discontent   \n",
       "1                                                                      i feel just bcoz a fight we get mad to each other n u wanna make a publicity n let the world knows about our fight   \n",
       "2                                                                                                                       i feel like reds and purples are just so rich and kind of perfect   \n",
       "3  im not sure the feeling of loss will ever go away but it may dull to a sweet feeling of nostalgia at what i shared in this life with my dad and the luck i had to have a dad for years   \n",
       "4                                        i feel like ive gotten to know many of you through comments and emails and for that im appreciative and glad you are a part of this little space   \n",
       "\n",
       "   feeling  \\\n",
       "0  sadness   \n",
       "1    anger   \n",
       "2      joy   \n",
       "3  sadness   \n",
       "4      joy   \n",
       "\n",
       "                                                                     transformed_text  \n",
       "0                                       feel like role tear sail pessimism discontent  \n",
       "1                      feel bcoz fight mad n u wanna publicity n let world know fight  \n",
       "2                                              feel like red purple rich kind perfect  \n",
       "3  m sure feeling loss away dull sweet feeling nostalgia share life dad luck dad year  \n",
       "4                feel like ve get know comment email m appreciative glad little space  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fine Tuning of Pre-Trained Transformer Model\n",
    "\n",
    "We will use the data processed with SpaCy and then perform the specific processing for the BERT model, just as we did with the model in version 1.\n",
    "\n",
    "https://huggingface.co/distilbert-base-multilingual-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the text into a sequence of integers for input to the BERT model\n",
    "def encode(texts, tokenizer, chunk_size = 256, max_len = 512):\n",
    "\n",
    "    # Enable truncation in the tokenizer to a specified max length\n",
    "    tokenizer.enable_truncation(max_length = max_len)\n",
    "\n",
    "    # Enable padding in th tokenizer to a specified max length\n",
    "    tokenizer.enable_padding(length = max_len)\n",
    "\n",
    "    # Initialize a list to stor the encoded IDs\n",
    "    all_ids = []\n",
    "\n",
    "    # Iterate over texts in chunk of size 'chunk_size'\n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "\n",
    "        # Create a chunk of text\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "\n",
    "        # Encode the chunk of text in batch\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "\n",
    "        # Extend the list 'all_ids' with the encoded IDs\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "\n",
    "    # Return the IDs list as an array numpy\n",
    "    return np.array(all_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olhar aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer from the pretrained model \n",
    "tokenizer_bert = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n",
    "# Save the tokenizer and the vocabulary locally\n",
    "tokenizer_bert.save_pretrained('.')\n",
    "\n",
    "# Load a faster tokenizer using the vocabulary of main tokenizer \n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase = False)\n",
    "\n",
    "# Show the tokenizer\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide os dados em treino e validação com amostragem estratificada\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df_train['transformed_text'].values,\n",
    "                                                        df_train['feeling'].values,\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state = 42,\n",
    "                                                        stratify = df_train['feeling'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified sampling is a technique used in statistics to ensure that subgroups (or strata) of a population are adequately represented within a sample. It is particularly useful in situations where the population is heterogeneous and the subgroups have different characteristics that are important to the research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 591.66it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 619.18it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 666.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12800, 100)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max lenght used in the text\n",
    "max_length = 100\n",
    "\n",
    "# Applying the encode in our data, using the faster tokenizer\n",
    "X_final_train = encode(X_train, fast_tokenizer, max_len = max_length)\n",
    "X_final_valid = encode(X_valid, fast_tokenizer, max_len = max_length)\n",
    "X_final_test = encode(df_test['transformed_text'].to_numpy(), fast_tokenizer, max_len = max_length)\n",
    "\n",
    "X_final_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder of output data\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Applying the label encoder (fit_transform only on train data)\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_valid_le = le.transform(y_valid)\n",
    "y_test_le = le.transform(df_test['feeling'])\n",
    "\n",
    "# Convert the output variable to categorical\n",
    "y_train_encoded = to_categorical(y_train_le)\n",
    "y_valid_encoded = to_categorical(y_valid_le)\n",
    "y_test_encoded = to_categorical(y_test_le)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Prepare the dataset in the expcted format of Pytorch\n",
    "train_dataset = TensorDataset(torch.tensor(X_final_train), torch.tensor(y_train_encoded))\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(torch.tensor(X_final_valid), torch.tensor(y_valid_encoded))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(X_final_test), torch.tensor(y_test_encoded))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Creates an instance of the pre-trained, multilingual DistilBERT model suitable for use with PyTorch\n",
    "transformer_model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, transformer, num_labels = 6):\n",
    "        super(Model, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.classifier = nn.Linear(transformer.config.hidden_size, num_labels)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Getting the output of sequence of transformer\n",
    "        outputs = self.transformer(input_ids)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        # Selecting the fist token of each sequence (token CLS from BERT) to classification\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        # Adding a dense layer to the output with softmax activation for classification\n",
    "        logits = self.classifier(cls_token)\n",
    "        out = self.softmax(logits)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (transformer): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model(transformer=transformer_model)\n",
    "\n",
    "for param in list(model.transformer.parameters())[:3]:\n",
    "    param.requires_grad = False \n",
    "\n",
    "# Defines the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 1/3: 100%|██████████| 800/800 [09:15<00:00,  1.44it/s, Loss (batch)=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/3]\n",
      "Loss of Training: 1.3304\n",
      "Validation Loss: 1.2456\n",
      "Training Accuracy: 0.2677\n",
      "Validation Accuracy: 0.7963\n",
      "Validation Precision: 0.7953\n",
      "Validation Recall: 0.7963\n",
      "Validation F1-Score: 0.7684\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 2/3: 100%|██████████| 800/800 [10:47<00:00,  1.24it/s, Loss (batch)=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/3]\n",
      "Loss of Training: 1.2211\n",
      "Validation Loss: 1.1936\n",
      "Training Accuracy: 0.2619\n",
      "Validation Accuracy: 0.8503\n",
      "Validation Precision: 0.8501\n",
      "Validation Recall: 0.8503\n",
      "Validation F1-Score: 0.8434\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 3/3: 100%|██████████| 800/800 [10:31<00:00,  1.27it/s, Loss (batch)=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/3]\n",
      "Loss of Training: 1.1792\n",
      "Validation Loss: 1.1805\n",
      "Training Accuracy: 0.2460\n",
      "Validation Accuracy: 0.8619\n",
      "Validation Precision: 0.8662\n",
      "Validation Recall: 0.8619\n",
      "Validation F1-Score: 0.8561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Progress_bar\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f'Training - Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "    for batch in train_loader_tqdm:\n",
    "        input_ids, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Converting predictions to classes (index)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        train_labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "        # Store predictions and labels\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(train_labels.cpu().numpy())\n",
    "\n",
    "        # Update progress bar with current average loss\n",
    "        train_loader_tqdm.set_postfix({'Loss (batch)': loss.item()})\n",
    "\n",
    "    # Average loss in training\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    # train_losses.append(train_loss)\n",
    "\n",
    "    # Calculate training metrics\n",
    "    train_accuracy = accuracy_score(y_train_le, all_preds)\n",
    "    train_precision = precision_score(y_train_le, all_preds, average='weighted')\n",
    "    train_recall = recall_score(y_train_le, all_preds, average='weighted')\n",
    "    train_f1 = f1_score(y_train_le, all_preds, average='weighted')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            input_ids, labels = batch\n",
    "            outputs = model(input_ids)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            # Converting predictions to classes (index)\n",
    "            val_labels = torch.argmax(labels, dim=1)\n",
    "            val_preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_val_preds.extend(val_preds.cpu().numpy())\n",
    "            all_val_labels.extend(val_labels.cpu().numpy())\n",
    "    # \n",
    "    val_loss_avg = val_running_loss / len(valid_loader)\n",
    "    # val_losses.append(val_loss_avg)\n",
    "\n",
    "    # Calculate validation metrics  \n",
    "    val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "    val_precision = precision_score(all_val_labels, all_val_preds, average='weighted')\n",
    "    val_recall = recall_score(all_val_labels, all_val_preds, average='weighted')\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
    "\n",
    "    # Print metrics after each epoch\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Loss of Training: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss_avg:.4f}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "    print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "    print(f\"Validation F1-Score: {val_f1:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Erro em Treino')\n",
    "plt.plot(val_losses, label='Erro em Validação')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "model.eval()\n",
    "\n",
    "# Converting X_final_test to a PyTorch tensor\n",
    "X_test_final_tensor = torch.tensor(X_final_test)\n",
    "\n",
    "# Predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_final_tensor)\n",
    "\n",
    "# Predicted labels (choosing the class index with highest probability)\n",
    "predicted_labels = torch.argmax(predictions, dim=1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86       275\n",
      "           1       0.84      0.87      0.85       224\n",
      "           2       0.86      0.95      0.90       695\n",
      "           3       0.91      0.53      0.67       159\n",
      "           4       0.88      0.92      0.90       581\n",
      "           5       0.86      0.55      0.67        66\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.87      0.78      0.81      2000\n",
      "weighted avg       0.87      0.87      0.86      2000\n",
      "\n",
      "[[231   5  10   3  25   1]\n",
      " [  9 195   3   1  16   0]\n",
      " [  4   7 659   2  18   5]\n",
      " [  3   1  61  85   9   0]\n",
      " [ 15   8  24   2 532   0]\n",
      " [  1  17  10   0   2  36]]\n",
      "0.869\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_le, predicted_labels))\n",
    "\n",
    "print(confusion_matrix(y_test_le, predicted_labels))\n",
    "\n",
    "print(accuracy_score(y_test_le, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving only weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (transformer): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Saving\n",
    "# Path where the model will be saved\n",
    "PATH = \"models/model_v3.pth\"\n",
    "\n",
    "# Saving only model weights\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "## Loading\n",
    "# Initializing the model (architecture must be the same as the saved model)\n",
    "model = Model(transformer_model)\n",
    "\n",
    "# Loaded the saved weights \n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# Put the model in evaluation mode if it is for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (transformer): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model, \"models/model_v3_complete.pth\")\n",
    "\n",
    "# Loading the complete model\n",
    "model = torch.load(\"models/model_v3_complete.pth\")\n",
    "\n",
    "# Putting the model in evaluation mode if it is for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 66.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# New sentence\n",
    "sentence = \"I'm happy today\"\n",
    "\n",
    "# Create a dataframe with the sentence\n",
    "df_new = pd.DataFrame({'text': [sentence]})\n",
    "\n",
    "# Applying the preprocessing function\n",
    "df_new['transformed_text'] = df_new['text'].apply(data_preprocessing)\n",
    "\n",
    "new_data = encode(df_new['transformed_text'], fast_tokenizer, max_len = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joy']\n"
     ]
    }
   ],
   "source": [
    "# Converting new_data to a PyTorch tensor if it isn't already\n",
    "new_data_tensor = torch.tensor(new_data)\n",
    "\n",
    "# Prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model(new_data_tensor)\n",
    "\n",
    "# Predicted labels (choosing the class index with highest probability)\n",
    "predicted_label = torch.argmax(prediction, dim=1).numpy()\n",
    "\n",
    "# Get the class name\n",
    "class_name = le.inverse_transform(predicted_label)\n",
    "print(class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
