{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks (FNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import spacy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 2)\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/train_data.txt', header=None, delimiter=';')\n",
    "df_test = pd.read_csv('data/test_data.txt', header=None, delimiter=';')\n",
    "\n",
    "df_train = df_train.rename(columns= {0: 'text', 1: 'feeling'})\n",
    "df_test = df_test.rename(columns= {0: 'text', 1: 'feeling'})\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train.head(3))\n",
    "display(df_test.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The column **text** is going to be the input feature and **feeling** is going to be the output target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['feeling'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['feeling'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing using Spacy\n",
    "\n",
    "[Oficial site](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dict\n",
    "spacy_nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(text):\n",
    "\n",
    "    doc = spacy_nlp(text)\n",
    "\n",
    "    tokens = [token.lemma_.lower().strip() for token in doc if not token.is_stop]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['transformed_text'] = df_train['text'].apply(data_preprocessing)\n",
    "df_test['transformed_text'] = df_test['text'].apply(data_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) TF-IDF Vectorizer\n",
    "\n",
    "Here is the English translation:\n",
    "\n",
    "This line above creates an instance of the **TfidfVectorizer** from the scikit-learn library, which is a tool used to convert a collection of raw documents into a TF-IDF (Term Frequency-Inverse Document Frequency) feature matrix. TF-IDF is a statistical technique used to quantify the importance of a word in a set of documents, commonly employed in natural language processing tasks and information retrieval.\n",
    "\n",
    "**Parameter max_df=0.95**: This parameter defines the maximum document frequency threshold for the terms to be considered. Here, it's set to 0.95, meaning that words appearing in more than 95% of the documents will be ignored. This helps eliminate common words that do not contribute much to the meaning of the text.\n",
    "\n",
    "**Parameter min_df=2**: This parameter establishes the minimum document frequency for the terms. In this case, terms that appear in fewer than two documents will be ignored. This helps filter out rare terms that may occur in only a few samples and are, therefore, less relevant to the overall analysis.\n",
    "\n",
    "**Parameter stop_words='english'**: This parameter instructs the vectorizer to remove all English stop words from the analysis. Stop words are common words (such as \"and,\" \"the,\" \"in\") that are usually filtered out in natural language processing because they are very frequent and do not carry significant information for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the vectorizer\n",
    "tf_idf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "df_train_tfidf = tf_idf.fit_transform(df_train['transformed_text'])\n",
    "df_test_tfidf = tf_idf.transform(df_test['transformed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverting the input data to array \n",
    "X_train_array = df_train_tfidf.toarray()\n",
    "X_test_array = df_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Data preparation\n",
    "\n",
    "We need to convert the target variable for numerical representation. We will use Label Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Doing the fit and transforming the target\n",
    "y_train_le = le.fit_transform(df_train['feeling'])\n",
    "y_test_le = le.transform(df_test['feeling'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's handle with class imbalance using the function `compute_class_weight`.\n",
    "\n",
    "**compute_class_weight**: This is a function from scikit-learn that calculates the weights for the classes. These weights can be used in classification models to give more importance to underrepresented classes in the dataset.\n",
    "\n",
    "**'balanced'**: This parameter indicates that the class weights should be computed in a way that balances the dataset. This is done inversely proportional to the frequency of the classes in the dataset. More frequent classes receive a lower weight, while less frequent classes receive a higher weight.\n",
    "\n",
    "**classes = np.unique(y_treino_le)**: Here, np.unique(y_treino_le) finds all the unique classes in the training dataset. The parameter `classes` informs the `compute_class_weight` function about these unique classes.\n",
    "\n",
    "**y = y_treino_le**: This is the label vector of the training dataset. The function will use these labels to calculate the frequency of each class.\n",
    "\n",
    "The result, stored in `pesos_classes`, is an array where each class has an associated weight. These weights can be used in classification models (such as a decision tree, logistic regression, SVM, etc.) to compensate for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights\n",
    "class_weight = compute_class_weight('balanced', classes = np.unique(y_train_le), y = y_train_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data spliting\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_array,\n",
    "                                                  y_train_le,\n",
    "                                                  test_size=0.2,\n",
    "                                                  stratify = y_train_le)\n",
    "\n",
    "# Converting the target to categorial\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test_le)\n",
    "y_val_encoded = to_categorical(y_val)\n",
    "\n",
    "# Shape\n",
    "y_train_encoded.shape, y_test_encoded.shape, y_val_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3) Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "\n",
    "# Initializing a sequential model. Sequential models are a linear stack of layers.\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Adding the first dense layer (fully-connected) to the model.\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(4096,\n",
    "                   # Using activation function SELU (Scaled Exponential Linear Unit)\n",
    "                   activation='selu',\n",
    "                   # Initializing the weights with Lecun normal distribution\n",
    "                   kernel_initializer='lecun_normal',\n",
    "                   # Defining the input shape\n",
    "                   input_shape=(X_train.shape[1],),\n",
    "                   # Applying regularizer L2 to reduce the overfitting\n",
    "                   kernel_regularizer = tf.keras.regularizers.l2(0.01)))\n",
    "\n",
    "# Adding the second dense layer\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(2048,\n",
    "                   activation='selu',\n",
    "                   kernel_initializer = 'lecun_normal',  \n",
    "                   kernel_regularizer = tf.keras.regularizers.l2(0.01)))  \n",
    "\n",
    "# Adding the third dense layer\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(1024,\n",
    "                   activation='selu',\n",
    "                   kernel_initializer='lecun_normal',\n",
    "                   kernel_regularizer=tf.keras.regularizers.l2(0.1)))\n",
    "\n",
    "# Adding the fourth dense layer\n",
    "# Layer with 64 neurons and activation SELU\n",
    "model.add(tf.keras.layers.Dense(64, \n",
    "                         activation='selu'))\n",
    "\n",
    "# Adding the output layer\n",
    "# Output layer with 6 neurons and softmax activation for multiclass classification\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(6,\n",
    "                   activation='softmax')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning specific weights to the bias array of the last layer of the model\n",
    "model.layers[-1].bias.assign(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compiling the model\n",
    "\n",
    "# Defining Adam optimizer\n",
    "# Adam is an optimization algorithm that can be used in place of the classical stochastic gradient descent \n",
    "# procedure to iteratively update network weights based on training data. It defines the loss function as 'categorical_crossentropy'. \n",
    "# It is suitable for multiclass classification problems, where labels are provided in a one-hot encoded format. \n",
    "# It defines the model evaluation metric as 'accuracy'. Accuracy is a common metric for evaluating the performance of classification models.\n",
    "model.compile(optimizer='Adam',\n",
    "              loss=tf.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Callbacks and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_rate = 0.001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_rate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate Scheduler\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Hyperparameters\n",
    "epochs_len = 20\n",
    "batch_size = 256\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train_encoded,\n",
    "    validation_data = (X_val, y_val_encoded),\n",
    "    epochs = epochs_len,\n",
    "    batch_size = batch_size,\n",
    "    callbacks = [early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracing the errors\n",
    "loss, val_loss = history.history['loss'], history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.plot(loss, label = 'loss')\n",
    "plt.plot(val_loss, label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with test data\n",
    "predictions = model.predict(X_test_array)\n",
    "\n",
    "# Extracting the labels\n",
    "predictions_labels = predictions.argmax(axis = 1)\n",
    "\n",
    "# Metrics\n",
    "print('Metrics')\n",
    "print(classification_report(y_test_le, predictions_labels))\n",
    "\n",
    "# Confusion matrix\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test_le, predictions_labels))\n",
    "\n",
    "# Accuracy\n",
    "print('Accuracy')\n",
    "print(accuracy_score(y_test_le, predictions_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model.save('models/model_v1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8) Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "loaded_model = tf.keras.models.load_model('models/model_v1.keras')\n",
    "\n",
    "# New sentence\n",
    "sentence = 'I am distressed because my girlfriend is sick'\n",
    "\n",
    "# Creating a dataframe with the sentence\n",
    "df_new = pd.DataFrame({'text': [sentence]})\n",
    "\n",
    "# Applying processing function\n",
    "df_new['transformed_text'] = df_new['text'].apply(data_preprocessing)\n",
    "\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing\n",
    "df_new_tfidf = tf_idf.transform(df_new['transformed_text'])\n",
    "\n",
    "# To array\n",
    "df_new_array = df_new_tfidf.toarray()\n",
    "\n",
    "# Previsões\n",
    "prediction = loaded_model.predict(df_new_array)\n",
    "\n",
    "# Seleciona a classe com maior probabilidade\n",
    "class_prob = np.argmax(prediction, axis = 1)\n",
    "\n",
    "# Obtém o nome da classe\n",
    "class_label = le.inverse_transform(class_prob)\n",
    "\n",
    "# Class predicted\n",
    "print(f'The feeling is {class_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
