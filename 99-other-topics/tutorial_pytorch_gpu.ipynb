{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple tutorial extracted from Chat GPT about how to use Pytorch with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute a PyTorch project using a GPU, you need to ensure that PyTorch is set up with CUDA (NVIDIA's parallel computing platform) and that your code explicitly uses the GPU. Hereâ€™s a step-by-step guide to help you run your PyTorch project on a GPU:\n",
    "\n",
    "## 1) Install PyTorch with GPU Support\n",
    "> First, you need to install the correct version of PyTorch that supports CUDA. You can install it using `pip` or `conda`, depending on your environment.\n",
    "\n",
    "> Using `pip`:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "\n",
    "> Here, cu118 refers to CUDA 11.8. Adjust this version based on your installed CUDA version.\n",
    "\n",
    "> Using `conda`:\n",
    "\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "```\n",
    "\n",
    "## 2) Check if CUDA is Available\n",
    "After installation, ensure that PyTorch can detect your GPU. You can do this by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! GPU name: NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available! GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CUDA is not available, check your installation of NVIDIA drivers, CUDA Toolkit, and PyTorch with GPU support.\n",
    "\n",
    "## 3) Move Your Model and Tensors to GPU\n",
    "To utilize the GPU, you'll need to move your model and data to the GPU using .to('cuda') or .cuda().\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0110], device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a simple model\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "# Move the model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Create a random input tensor and move it to GPU\n",
    "input_tensor = torch.randn(10).to(device)\n",
    "\n",
    "# Forward pass (on GPU)\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "* The model and tensor are moved to the GPU using to(device), where device is set to 'cuda' if available.\n",
    "* Any computation that happens after this will utilize the GPU.\n",
    "\n",
    "## 4) Training a Model on the GPU\n",
    "When training a model, you should also move the data (inputs and labels), the model, and the loss function to the GPU.\n",
    "\n",
    "Example of a simple training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.1432\n",
      "Epoch [20/100], Loss: 1.0414\n",
      "Epoch [30/100], Loss: 0.9763\n",
      "Epoch [40/100], Loss: 0.9343\n",
      "Epoch [50/100], Loss: 0.9068\n",
      "Epoch [60/100], Loss: 0.8885\n",
      "Epoch [70/100], Loss: 0.8763\n",
      "Epoch [80/100], Loss: 0.8679\n",
      "Epoch [90/100], Loss: 0.8621\n",
      "Epoch [100/100], Loss: 0.8581\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a simple model and move it to the GPU\n",
    "model = nn.Linear(10, 1).to(device)\n",
    "\n",
    "# Define a loss function and an optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create some random data (100 samples, 10 features) and move to GPU\n",
    "inputs = torch.randn(100, 10).to(device)\n",
    "targets = torch.randn(100, 1).to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Key Things to Remember:\n",
    "1. Move everything to the GPU: Ensure that the model, input tensors, labels, and any intermediate tensors are moved to the GPU by calling .to(device) where device is set to 'cuda' when available.\n",
    "2. Check CUDA availability: Always check if CUDA is available using torch.cuda.is_available() to avoid errors on systems without a GPU.\n",
    "3. Utilize Multiple GPUs: If you have multiple GPUs, you can use torch.nn.DataParallel or torch.nn.parallel.DistributedDataParallel to distribute the model across them.\n",
    "\n",
    "\n",
    "Using Multiple GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=1, bias=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Useful PyTorch CUDA Commands:\n",
    "* Check number of available GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the name of the current GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear GPU memory (useful after debugging):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Example Full Workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.0222\n",
      "Epoch [20/100], Loss: 0.9496\n",
      "Epoch [30/100], Loss: 0.8930\n",
      "Epoch [40/100], Loss: 0.8436\n",
      "Epoch [50/100], Loss: 0.7959\n",
      "Epoch [60/100], Loss: 0.7472\n",
      "Epoch [70/100], Loss: 0.6965\n",
      "Epoch [80/100], Loss: 0.6452\n",
      "Epoch [90/100], Loss: 0.5932\n",
      "Epoch [100/100], Loss: 0.5410\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a simple model and move it to the GPU\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Random data\n",
    "inputs = torch.randn(100, 10).to(device)\n",
    "targets = torch.randn(100, 1).to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "        \n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
