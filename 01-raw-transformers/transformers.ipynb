{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attencion Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um modelo capaz de prever sequências de comprimento igual a 10 tokens.\n",
    "\n",
    "Um modelo Transformer consiste em várias partes principais:\n",
    "* 1 - Camada de Embedding: Teransforma as palavras em vetores numéricos de tamanho fixo.\n",
    "* 2 - Mecanismo de Atenção: Permite o modelo foque em diferentes partes da entrada.\n",
    "* 3 - Camadas Encoder e Decoder: Processam os dados sequencialmente.\n",
    "* 4 - Camada Linear e Softmax: Para predições finais.\n",
    "\n",
    "Para este projeto o grande objetivo é implementar o item 2, mas para deixar o exemplo funcional, implementarei também os itens 1 e 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camada Embedding\n",
    "\n",
    "A função embedding é utilizada para converter entradas sequenciais em vetores densos de tamanho fixo. Esses vetores são conhecidos como embeddings e são uma parte fundamental, em especial dos modelos de PLN. \n",
    "\n",
    "Esses embeddings são fundamentais para modelos de aprendizado profundo em PLN, pois fornecem uma representação rica e densa de palavras ou tokens, capturando informações contextuais e semânticas que são essenciais para tarefas como tradução automática, classificação de texto, entre outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função para criar uma matriz de embedding\n",
    "def embedding(input, vocab_size, dim_model):\n",
    "\n",
    "    # Cria a matriz de embedding onde cada linha representa um token do vocabulário\n",
    "    # A matriz é inicializada com valores aleatórios normalmente distribuídos\n",
    "    embed = np.random.randn(vocab_size, dim_model)\n",
    "\n",
    "    # Para cada índice de token no input, seleciona o embedding correspondente da matriz\n",
    "    # Retorna um array de embeddings correspondentes à sequência de entrada\n",
    "    return np.array([embed[i] for i in input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecanismo de Atenção\n",
    "\n",
    "No Transformer, Q, K e V são derivados da mesma entrada em camadas de atenção do encoder, mas de entradas diferentes no decoder (Q vem da saída da camada anterior do decoder, enquanto K e V vêm da saída do encoder). O mecanismo de atenção calcula um conjunto de pontuações (usando o produto escalar entre Q e K, daí o nome \"scaled dot-product attention\"), aplica uma função softmax para obter pesos de atenção e usa esses pesos para ponderar os values, criando uma saída que é uma combinação ponderada das informações relevantes de entrada.\n",
    "\n",
    "Este processo permite que o modelo dê \"atenção\" às partes mais relevantes da entrada para cada parte da saída, o que é especialmente útil em tarefas como tradução, onde a relevância de diferentes palavras da entrada pode variar dependendo da parte da frase que está sendo traduzida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Ativação Softmax\n",
    "\n",
    "A função softmax é uma função de ativação amplamente utilizada em redes neurais, especialmente em cenários de classificação, onde é importante transformar valores brutos de saída (logits) em probabilidades que somam 1. Abaixo, está o código da função softmax com comentários em cada linha explicando seu funcionamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de ativação softmax\n",
    "def softmax(x):\n",
    "    \n",
    "    # Calcula o exponencial de cada elemento do input, ajustado pelo máximo valor no input \n",
    "    # para evitar overflow numérico\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    \n",
    "    # Divide cada exponencial pelo somatório dos exponenciais ao longo do último eixo (axis=-1)\n",
    "    # O reshape(-1, 1) garante que a divisão seja realizada corretamente em um contexto multidimensional\n",
    "    return e_x / e_x.sum(axis=-1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Dot Product\n",
    "\n",
    "A função scaled_dot_product_attention() é um componente do mecanismo de atenção em modelos Transformer. Ela calcula a atenção entre conjuntos de queries (Q), keys (K) e values (V). \n",
    "\n",
    "Essencialmente, essa função permite que o modelo dê importância diferenciada a diferentes partes da entrada, um aspecto chave que torna os modelos Transformer particularmente eficazes para tarefas de PLN e outras tarefas sequenciais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função para calcular a atenção escalada por produto escalar\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \n",
    "    # Calcula o produto escalar entre Q e a transposta de K\n",
    "    matmul_qk = np.dot(Q, K.T)\n",
    "    \n",
    "    # Obtém a dimensão dos vetores de chave\n",
    "    depth = K.shape[-1]\n",
    "    \n",
    "    # Escala os logits dividindo-os pela raiz quadrada da profundidade\n",
    "    logits = matmul_qk / np.sqrt(depth)\n",
    "    \n",
    "    # Aplica a função softmax para obter os pesos de atenção\n",
    "    attention_weights = softmax(logits)\n",
    "    \n",
    "    # Multiplica os pesos de atenção pelos valores V para obter a saída final\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    # Retorna a saída ponderada\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saída do Modelo com Operação Linear e Softmax\n",
    "\n",
    "A função linear_and_softmax() é uma combinação de uma camada linear seguida por uma função softmax, comumente usada em modelos de aprendizado profundo, especialmente em tarefas de classificação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função que aplica uma transformação linear seguida de softmax\n",
    "def linear_and_softmax(input):\n",
    "    \n",
    "    # Inicializa uma matriz de pesos com valores aleatórios normalmente distribuídos\n",
    "    # Esta matriz conecta cada dimensão do modelo (dim_model) a cada palavra do vocabulário (vocab_size)\n",
    "    weights = np.random.randn(dim_model, vocab_size)\n",
    "    \n",
    "    # Realiza a operação linear (produto escalar) entre a entrada e a matriz de pesos\n",
    "    # O resultado, logits, é um vetor que representa a entrada transformada em um espaço de maior dimensão\n",
    "    logits = np.dot(input, weights)\n",
    "    \n",
    "    # Aplica a função softmax aos logits\n",
    "    # Isso transforma os logits em um vetor de probabilidades, onde cada elemento soma 1\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo o Modelo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função do modelo final\n",
    "def transformer_model(input):\n",
    "    \n",
    "    # Embedding\n",
    "    embedded_input = embedding(input, vocab_size, dim_model)\n",
    "\n",
    "    # Mecanismo de Atenção \n",
    "    attention_output = scaled_dot_product_attention(embedded_input, embedded_input, embedded_input)\n",
    "    \n",
    "    # Camada linear e softmax\n",
    "    output_probabilities = linear_and_softmax(attention_output)\n",
    "\n",
    "    # Escolhendo os índices com maior probabilidade\n",
    "    output_indices = np.argmax(output_probabilities, axis=-1)\n",
    "    \n",
    "    return output_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Hiperparâmetros iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensão do modelo\n",
    "dim_model = 4\n",
    "\n",
    "# Comprimento da sequência\n",
    "seq_length = 5\n",
    "\n",
    "# Tamanho do vocabulário\n",
    "vocab_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usando o Modelo Para as Previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dados aleatórios para a entrada do modelo\n",
    "input_sequence = np.random.randint(0, vocab_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequência de Entrada: [69 88 29 87 67]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequência de Entrada:\", input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo previsões com o modelo\n",
    "output = transformer_model(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída do Modelo: [29 45 45 29 70]\n"
     ]
    }
   ],
   "source": [
    "print(\"Saída do Modelo:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo a passo da execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69, 88, 29, 87, 67])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gerando dados aleatórios para a entrada do modelo\n",
    "input_sequence = np.random.randint(0, vocab_size, seq_length)\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2168222 ,  1.30473414,  0.84480715, -1.35817327]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding\n",
    "embedded_input = embedding(input_sequence, vocab_size, dim_model)\n",
    "embedded_input[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.86992256,  1.10027359,  1.00464841, -1.08533723],\n",
       "       [-0.92628395,  0.25245825, -0.65779795,  0.30722767],\n",
       "       [-0.18069875,  0.56752686, -0.21765976,  0.17738769],\n",
       "       [-0.60207574,  1.08489145,  1.1428699 , -0.95905918],\n",
       "       [ 1.59459658,  1.25569574,  2.27023003, -0.06384452]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mecanismo de Atenção \n",
    "attention_output = scaled_dot_product_attention(embedded_input, embedded_input, embedded_input)\n",
    "attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.81159921e-03, 5.97001201e-03, 1.69792926e-03, 2.81770553e-04,\n",
       "        2.79675701e-02, 1.35030199e-03, 6.04385815e-03, 1.28588590e-02,\n",
       "        5.74703887e-03, 5.08303414e-03, 1.85165348e-01, 7.22589511e-02,\n",
       "        2.20139299e-03, 1.85332595e-03, 1.69767761e-03, 6.68713649e-03,\n",
       "        7.47125021e-04, 1.11479663e-03, 3.34936017e-05, 2.60155492e-03,\n",
       "        8.95294771e-03, 4.23934888e-04, 1.81900494e-03, 1.16625039e-02,\n",
       "        5.44774381e-03, 1.91159052e-03, 5.64731639e-03, 3.35465872e-04,\n",
       "        4.89316961e-02, 1.20067010e-03, 7.45868378e-04, 1.58248462e-03,\n",
       "        8.74595059e-04, 3.11512037e-03, 1.68693105e-02, 2.25706713e-03,\n",
       "        3.04682834e-04, 9.67517185e-06, 2.89540808e-04, 8.13229494e-04,\n",
       "        5.52225768e-04, 3.87057710e-03, 1.96601758e-04, 1.02986887e-02,\n",
       "        7.06578368e-02, 2.73706206e-02, 3.77780301e-03, 1.54753011e-03,\n",
       "        1.07286219e-02, 1.16311981e-03, 2.10905909e-04, 6.53756345e-04,\n",
       "        3.02549000e-03, 4.21814010e-02, 1.53654872e-02, 3.92005384e-05,\n",
       "        4.98125336e-04, 1.55330179e-04, 2.61991814e-03, 2.47696822e-04,\n",
       "        4.45229329e-04, 4.74049138e-04, 4.07732129e-03, 3.40590701e-03,\n",
       "        2.20263389e-03, 3.02312618e-04, 2.41480457e-04, 4.72066286e-04,\n",
       "        1.19184119e-01, 6.32051712e-03, 2.96605503e-03, 2.36638154e-03,\n",
       "        4.91986038e-04, 1.62129994e-02, 1.97952753e-04, 1.95909470e-02,\n",
       "        3.63000781e-03, 2.65240899e-03, 3.57523927e-02, 2.16054195e-04,\n",
       "        5.09089677e-05, 2.31583126e-04, 5.52631052e-03, 8.46117537e-03,\n",
       "        2.64078688e-03, 5.45523127e-04, 1.47085263e-03, 1.16604784e-03,\n",
       "        5.51934250e-04, 1.81988647e-04, 7.52424221e-04, 1.19883181e-04,\n",
       "        2.12793518e-03, 9.11388297e-04, 1.41981817e-04, 9.14194191e-02,\n",
       "        1.56972763e-04, 1.04116565e-02, 2.92530592e-04, 1.07714389e-04],\n",
       "       [2.44468369e-03, 9.13780276e-05, 1.59292456e-03, 4.99514521e-03,\n",
       "        2.21224164e-03, 9.52398500e-03, 9.76239442e-02, 1.25378853e-03,\n",
       "        4.59237876e-02, 7.58910328e-03, 1.48963871e-02, 5.39188301e-03,\n",
       "        1.02064692e-02, 1.01678402e-02, 5.14923712e-03, 2.98219129e-03,\n",
       "        1.68766256e-03, 5.61009825e-03, 1.66061849e-02, 4.00521812e-03,\n",
       "        1.44164239e-03, 1.28384392e-02, 1.30385112e-02, 3.80685421e-03,\n",
       "        1.72244857e-02, 3.77740797e-03, 2.27083401e-03, 1.02808103e-02,\n",
       "        3.17601558e-03, 2.12379269e-02, 5.52279305e-03, 1.23951691e-02,\n",
       "        5.33565695e-04, 6.15819690e-03, 2.69735356e-03, 3.23435350e-03,\n",
       "        1.74346619e-03, 4.13427643e-03, 1.42519039e-02, 5.63786986e-03,\n",
       "        1.55252568e-03, 1.48561318e-02, 4.97658902e-03, 3.59708054e-03,\n",
       "        1.15953809e-02, 6.90375300e-03, 7.90982641e-02, 1.05710873e-02,\n",
       "        2.16773984e-03, 1.21523088e-03, 9.56923111e-03, 2.24710438e-02,\n",
       "        4.21620002e-03, 8.48560219e-03, 5.75004043e-04, 3.48839059e-03,\n",
       "        1.09177608e-02, 4.00679713e-03, 1.66016472e-02, 8.44696603e-03,\n",
       "        2.47162671e-03, 2.18280827e-02, 4.33412597e-03, 1.46197534e-03,\n",
       "        1.24348256e-03, 5.41935487e-04, 1.16860606e-03, 2.97489293e-03,\n",
       "        1.43588385e-02, 8.05635432e-03, 7.35162671e-03, 1.25179748e-03,\n",
       "        8.87786516e-04, 2.74442074e-03, 2.49910487e-03, 5.62449215e-03,\n",
       "        2.33707280e-03, 1.31865090e-02, 2.01373766e-03, 1.27220002e-03,\n",
       "        1.22652581e-02, 1.92631637e-02, 1.48192519e-02, 1.63345599e-01,\n",
       "        7.89842042e-03, 7.53356809e-04, 4.25543469e-03, 1.87201730e-03,\n",
       "        4.37220619e-03, 5.98041877e-03, 2.48230536e-03, 5.81284254e-04,\n",
       "        3.44577986e-03, 1.40822742e-03, 1.25119555e-02, 8.84325946e-03,\n",
       "        1.59301689e-03, 4.30772024e-03, 8.67972046e-03, 3.04448043e-03],\n",
       "       [6.78532431e-03, 3.41526801e-03, 7.66303396e-03, 8.56912183e-03,\n",
       "        8.49251510e-03, 3.56520222e-02, 1.71820258e-02, 4.23315840e-03,\n",
       "        1.48777428e-02, 9.00274318e-03, 2.46274695e-02, 2.19231051e-02,\n",
       "        1.96567492e-02, 6.73680813e-03, 8.40648999e-03, 4.23533290e-03,\n",
       "        2.52256420e-03, 8.48413538e-03, 5.13114331e-03, 5.57309743e-03,\n",
       "        3.24431675e-03, 8.06723995e-03, 7.78078625e-03, 3.89667429e-03,\n",
       "        1.39133933e-02, 5.61170249e-03, 1.49700883e-02, 6.35773080e-03,\n",
       "        9.22988503e-03, 1.14035413e-02, 1.88399386e-02, 6.82033026e-03,\n",
       "        2.92498971e-03, 1.01097752e-02, 2.74461498e-02, 1.09621429e-02,\n",
       "        4.71381170e-03, 4.01191087e-03, 7.07802280e-03, 6.57749868e-03,\n",
       "        3.43221835e-03, 3.48557366e-02, 1.92174161e-03, 8.52920860e-03,\n",
       "        3.08134665e-02, 7.72925116e-03, 2.47745442e-02, 1.29061008e-02,\n",
       "        8.53893047e-03, 6.59133187e-03, 6.83700666e-03, 8.18767607e-03,\n",
       "        7.71929306e-03, 1.92764741e-02, 7.41242571e-03, 4.76010482e-03,\n",
       "        6.51070694e-03, 2.61548004e-03, 9.61033358e-03, 1.19671740e-02,\n",
       "        6.44186789e-03, 1.39130367e-02, 4.76670103e-03, 4.40878473e-03,\n",
       "        1.23955956e-02, 6.25719867e-03, 5.64735248e-03, 5.25915093e-03,\n",
       "        2.62229620e-02, 1.34316976e-02, 1.32199978e-02, 5.50481663e-03,\n",
       "        4.69969543e-03, 8.65339882e-03, 1.05815002e-02, 1.18993721e-02,\n",
       "        4.58205816e-03, 1.49722315e-02, 1.14106552e-02, 4.60113616e-03,\n",
       "        7.80050360e-03, 9.37211659e-03, 1.84345007e-02, 2.91862008e-02,\n",
       "        1.14690550e-02, 1.12167537e-02, 5.30964911e-03, 7.74574076e-03,\n",
       "        5.33850088e-03, 7.71774084e-03, 6.12375261e-03, 4.35566608e-03,\n",
       "        6.59408718e-03, 6.59996952e-03, 8.33833363e-03, 1.27745601e-02,\n",
       "        4.72027073e-03, 6.43724683e-03, 7.36287420e-03, 2.11428255e-03],\n",
       "       [2.85455412e-03, 1.51035922e-02, 2.69494421e-03, 3.95003971e-04,\n",
       "        3.11886396e-02, 1.95526699e-03, 3.16728169e-03, 1.71808642e-02,\n",
       "        4.52204220e-03, 5.09392039e-03, 1.41671069e-01, 7.63943001e-02,\n",
       "        2.53230860e-03, 1.45161801e-03, 1.62248497e-03, 5.65315634e-03,\n",
       "        9.81468539e-04, 1.14769243e-03, 2.81918239e-05, 3.15302218e-03,\n",
       "        1.03489280e-02, 4.16941852e-04, 1.42842008e-03, 1.15941817e-02,\n",
       "        4.32709549e-03, 1.63261129e-03, 6.63448062e-03, 3.23138900e-04,\n",
       "        4.57215119e-02, 9.13032984e-04, 1.14734464e-03, 1.37941038e-03,\n",
       "        1.72783364e-03, 3.25680174e-03, 2.90001472e-02, 3.90299175e-03,\n",
       "        3.67661750e-04, 1.21267431e-05, 2.86750500e-04, 1.07274890e-03,\n",
       "        8.64272834e-04, 4.77258844e-03, 1.70127380e-04, 1.00242152e-02,\n",
       "        8.04351571e-02, 1.99083239e-02, 2.42506843e-03, 1.41361444e-03,\n",
       "        1.51629169e-02, 1.82274079e-03, 2.53167376e-04, 4.12479670e-04,\n",
       "        3.49466121e-03, 3.99329785e-02, 3.27694744e-02, 6.88729602e-05,\n",
       "        5.12451247e-04, 1.68840982e-04, 2.58799613e-03, 3.28819029e-04,\n",
       "        8.03033374e-04, 4.76086517e-04, 4.77272713e-03, 4.68708429e-03,\n",
       "        5.04901167e-03, 6.93718257e-04, 5.86124538e-04, 5.51667732e-04,\n",
       "        9.68303920e-02, 6.84461249e-03, 3.40833171e-03, 3.28642200e-03,\n",
       "        8.72375595e-04, 1.62447202e-02, 3.46790447e-04, 2.32213441e-02,\n",
       "        3.85075587e-03, 2.10334077e-03, 5.44398124e-02, 3.31227965e-04,\n",
       "        6.10999669e-05, 2.52820348e-04, 5.12111563e-03, 4.30775767e-03,\n",
       "        2.94601682e-03, 1.53780942e-03, 1.77108670e-03, 1.80383074e-03,\n",
       "        5.78571224e-04, 2.13820965e-04, 9.95785715e-04, 2.33307190e-04,\n",
       "        2.44299054e-03, 1.70974990e-03, 1.47181209e-04, 7.41458957e-02,\n",
       "        3.06582558e-04, 9.75891206e-03, 3.20461154e-04, 1.31276562e-04],\n",
       "       [1.16067436e-03, 4.72499773e-01, 1.50774928e-03, 5.97478623e-05,\n",
       "        1.29288248e-03, 7.51880849e-04, 1.84219060e-07, 2.06530045e-03,\n",
       "        6.10340348e-06, 5.99714459e-05, 3.59363398e-04, 2.82124246e-03,\n",
       "        1.20490906e-04, 2.10284001e-06, 1.52914574e-05, 1.74869766e-05,\n",
       "        6.43067636e-05, 1.78919129e-05, 4.58340074e-08, 1.37691834e-04,\n",
       "        3.44898352e-04, 3.16611268e-06, 2.09081971e-06, 9.77939929e-05,\n",
       "        9.09174317e-06, 5.94409140e-06, 5.81577116e-04, 1.97453745e-06,\n",
       "        4.82630506e-04, 1.11718872e-06, 5.55840055e-04, 3.98987733e-06,\n",
       "        3.84949597e-03, 6.15037587e-05, 5.51261373e-02, 3.81120062e-03,\n",
       "        1.97778818e-05, 5.27523591e-07, 2.00029127e-06, 8.31454932e-05,\n",
       "        2.54784134e-04, 4.71352683e-04, 2.89665984e-07, 1.27196303e-04,\n",
       "        4.59117956e-03, 2.20529204e-05, 8.51066021e-07, 9.48108881e-06,\n",
       "        3.48589665e-03, 9.42468607e-04, 8.14140377e-06, 1.02666936e-07,\n",
       "        1.32098460e-04, 5.02590532e-04, 2.32219414e-01, 4.10409114e-05,\n",
       "        4.97761196e-06, 2.00349987e-06, 2.05164259e-05, 3.37240127e-05,\n",
       "        8.26271336e-04, 4.85831134e-06, 1.41108489e-04, 6.69469782e-04,\n",
       "        6.12020769e-02, 7.56304566e-03, 6.40923351e-03, 2.02092524e-05,\n",
       "        4.00682815e-04, 1.80269418e-04, 1.41296274e-04, 6.23408226e-04,\n",
       "        9.93446497e-04, 2.88041231e-04, 4.04234892e-04, 1.25845716e-03,\n",
       "        7.12156238e-05, 5.14599440e-06, 2.69448496e-02, 1.11930363e-04,\n",
       "        1.92746674e-06, 3.77961663e-06, 4.21050276e-05, 2.34650271e-07,\n",
       "        8.68559422e-05, 9.51959532e-02, 6.95282164e-05, 7.88475015e-04,\n",
       "        8.31645997e-06, 7.83423437e-06, 1.06981911e-04, 5.89238518e-04,\n",
       "        8.65008430e-05, 3.02289761e-03, 1.68434258e-06, 2.28047051e-04,\n",
       "        5.44737380e-04, 7.20957462e-05, 5.86392936e-06, 3.44505892e-06]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Camada linear e softmax\n",
    "output_probabilities = linear_and_softmax(attention_output)\n",
    "output_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 83,  5, 10,  1], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Escolhendo os índices com maior probabilidade\n",
    "output_indices = np.argmax(output_probabilities, axis=-1)\n",
    "output_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
