{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning of Open-Source LLM with Custom Dataset for Text Generation\n",
    "\n",
    "Using a pre-trained open-source LLM, we will fine-tune the model with our own data, adjusting the LLM for a specific task: generating medical text in response to a patient's clinical description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate peft bitsandbytes trl datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import TrainingArguments\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU count: 1\n",
      "GPU model: NVIDIA GeForce RTX 4060 Ti\n",
      "Total GPU Memory [GB]: 8.585216\n"
     ]
    }
   ],
   "source": [
    "# Verifica o modelo da GPU\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU count:', torch.cuda.device_count())\n",
    "    print('GPU model:', torch.cuda.get_device_name(0))\n",
    "    print('Total GPU Memory [GB]:',torch.cuda.get_device_properties(0).total_memory / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU memory reset (when needed)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259a08307adb4cf0b834b521d93ff6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3fa257fb4c402cbec19315658863a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-a8790d88efc2bc45.parquet:   0%|          | 0.00/91.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b229cde3676f4368ac049d5647a8bcb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-b543c64b1786c03e.parquet:   0%|          | 0.00/6.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0adea54fe7d4a17a69314a60452208c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/200252 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0758136525c84074a5ab768d2986ad8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/70066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carrega o dataset\n",
    "dataset = load_dataset(\"nlpie/Llama2-MedTuned-Instructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'source'],\n",
      "        num_rows: 200252\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'source'],\n",
      "        num_rows: 70066\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "sys.getsizeof(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Point 1:\n",
      "Instruction: In your role as a medical professional, address the user's medical questions and concerns.\n",
      "Input: My relative suffering from secondary lever cancer ( 4th stage as per Allopathic doctor) and primary is in rectum. He is continuously with 103 to 104 degree F fever. Allpathic doctor suggested chemo only after fever subsidises. Is treatment possible at Lavanya & what is the time scale of recover.\n",
      "Output: Hi, dairy have gone through your question. I can understand your concern. He has rectal cancer with liver metastasis. It is stage 4 cancer. Surgery is not possible at this stage. Only treatment options are chemotherapy and radiotherapy according to type of cancer. Inspite of all treatment prognosis is poor. Life expectancy is not good. Consult your doctor and plan accordingly. Hope I have answered your question, if you have any doubts then contact me at bit.ly/ Chat Doctor. Thanks for using Chat Doctor. Wish you a very good health.\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Data Point 2:\n",
      "Instruction: Your role as a doctor requires you to answer the medical questions taking into account the patient's description.\n",
      "Analyze the question given its context. Give both long answer and yes/no decision.\n",
      "Input: ###Question: Are fibrocytes involved in inflammation as well as fibrosis in the pathogenesis of Crohn 's disease?\n",
      "###Context: We previously showed that fibrocytes, a hematopoietic stem cell source of fibroblasts/myofibroblasts, infiltrated the colonic mucosa of a murine colitis model. We investigated whether fibrocytes were involved in the pathogenesis of Crohn's disease. Human surgical intestinal specimens were stained with anti-leukocyte-specific protein 1 and anti-collagen type-I (ColI) antibodies. Circulating fibrocytes in the human peripheral blood were quantified by fluorescence-activated cell sorting with anti-CD45 and anti-ColI antibodies. Cultured human fibrocytes were prepared by culturing peripheral CD14(+) monocytes. In the specimens of patients with Crohn's disease, the fibrocyte/total leukocyte percentage was significantly increased in inflammatory lesions (22.2 %, p < 0.01) compared with that in non-affected areas of the intestine (2.5 %). Interestingly, the percentage in fibrotic lesions was similar (2.2 %, p = 0.87) to that in non-affected areas. The percentages of circulating fibrocytes/total leukocytes were significantly higher in patients with Crohn's disease than in healthy controls. Both CXC-chemokine receptor 4(+) and intercellular adhesion molecule 1(+) fibrocyte numbers were significantly increased in Crohn's disease, suggesting that circulating fibrocytes have a higher ability to infiltrate injured sites and traffic leukocytes. In cultured fibrocytes, lipopolysaccharide treatment remarkably upregulated tumor necrosis factor (TNF)-α mRNA (17.0 ± 5.7-fold) and ColI mRNA expression (12.8 ± 5.7-fold), indicating that fibrocytes stimulated by bacterial components directly augmented inflammation as well as fibrosis.\n",
      "Output: Fibrocytes are recruited early in the inflammatory phase and likely differentiate into fibroblasts/myofibroblasts until the fibrosis phase. They may enhance inflammation by producing TNF-α and can directly augment fibrosis by producing ColI.\n",
      "\n",
      "###Answer: yes\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Data Point 3:\n",
      "Instruction: Your identity is a doctor, kindly provide answers to the medical questions with consideration of the patient's description.\n",
      "Analyze the question and answer with the best option.\n",
      "Input: ###Question: Afterhyperpolarization due to\n",
      "###Options:\n",
      "A. Na efflux\n",
      "B. Na+ influx\n",
      "C. CI influx\n",
      "D. K+ efflux\n",
      "\n",
      "Output: ###Rationale: Slow return of the K+ channels to the closed state thus K+ efflux.(Ref: Textbook of physiology AK Jain 5th edition page no.36)\n",
      "\n",
      "###Answer: OPTION D IS CORRECT.\n",
      "\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop to view the first 3 records\n",
    "for i in range(3):\n",
    "    data = dataset['train'][i]\n",
    "    print(f\"Data Point {i + 1}:\")\n",
    "    print(\"Instruction:\", data['instruction'])\n",
    "    print(\"Input:\", data['input'])\n",
    "    print(\"Output:\", data['output'])\n",
    "    print(\"\\n-----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work with smaller samples (to speed ​​up model training time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sample\n",
    "dataset[\"train\"] = dataset[\"train\"].select(range(3500))\n",
    "\n",
    "# Test sample\n",
    "dataset[\"test\"] = dataset[\"train\"].select(range(300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the Input Prompt Format for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criação do prompt\n",
    "def create_prompt(sample):\n",
    "    prompt = sample['instruction']\n",
    "    prompt += sample['input']\n",
    "    single_turn_prompt = f\"Instruction: {prompt}<|end_of_turn|>AI Assistant: {sample['output']}\"\n",
    "    return single_turn_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: In your role as a medical professional, address the user's medical questions and concerns.My relative suffering from secondary lever cancer ( 4th stage as per Allopathic doctor) and primary is in rectum. He is continuously with 103 to 104 degree F fever. Allpathic doctor suggested chemo only after fever subsidises. Is treatment possible at Lavanya & what is the time scale of recover.<|end_of_turn|>AI Assistant: Hi, dairy have gone through your question. I can understand your concern. He has rectal cancer with liver metastasis. It is stage 4 cancer. Surgery is not possible at this stage. Only treatment options are chemotherapy and radiotherapy according to type of cancer. Inspite of all treatment prognosis is poor. Life expectancy is not good. Consult your doctor and plan accordingly. Hope I have answered your question, if you have any doubts then contact me at bit.ly/ Chat Doctor. Thanks for using Chat Doctor. Wish you a very good health.\n"
     ]
    }
   ],
   "source": [
    "# Example of using the function\n",
    "print(create_prompt(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Parameters\n",
    "\n",
    "Large Language Model (LLM) quantization is a process of reducing the precision of a large language model (LLM)'s parameters to a lower value without losing much of its performance. This is done to reduce the complexity of the model and consequently improve computational efficiency and storage efficiency.\n",
    "\n",
    "LLM quantization is useful in several situations, such as:\n",
    "\n",
    "- Reducing model size: Quantization can reduce the size of the model, which can improve storage and data transfer efficiency.\n",
    "\n",
    "- Improving computational efficiency: Quantization can improve the computational efficiency of the model, making it faster and more efficient in terms of computational resources.\n",
    "\n",
    "- Increasing scalability: Quantization can allow models to be trained on cheaper and more efficient hardware, which can be useful in production applications.\n",
    "\n",
    "BitsAndBytesConfig is used to configure LLM quantization. It is an object that contains information about quantization, such as:\n",
    "\n",
    "- Quantization type: the type of quantization to use, such as floating-point quantization or integer quantization.\n",
    "\n",
    "- Number of bits: the number of bits to use to represent the model parameters.\n",
    "\n",
    "BitsAndBytesConfig is used to configure LLM quantization and can be used in conjunction with other parameters to train a more efficient and scalable language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the quantization parameters\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit = True,\n",
    "                                bnb_4bit_quant_type = \"nf4\",\n",
    "                                bnb_4bit_compute_dtype = \"float16\",\n",
    "                                bnb_4bit_use_double_quant = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the LLM and Tokenizer\n",
    "\n",
    "https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32236dfa25cd49c795ee5ace6c3833cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be252b3ccaef44889437a0b4bf22d8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b03f9c476346c8b15fe3d2197984bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289c6ae004324b0d87d1624b88c66e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13791848351b49619abde5d2bbfadad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b52f2debff4da09f668fc3c786909b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe7485b256b4f9694deb5b49e3daced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de388c2d5e64d2e9a58ad32fec449d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Repository name on Hugging Face\n",
    "repository_hf = \"berkeley-nest/Starling-LM-7B-alpha\"\n",
    "\n",
    "# Load the LLM applying quantization\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(repository_hf,\n",
    "                                                  quantization_config = bnb_config,\n",
    "                                                  device_map = \"auto\",\n",
    "                                                  use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a0c59865df4e59ba32c8aa7f7e3484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3ef9a7ceb84aa0815a43ad3ae382cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa6aa2ed24d445c9d59df6bbf45ac91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c12f5282f242359ef1cf4f1adc5eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfdae3ad02b4351aba4336d35a8aa7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the LLM tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(repository_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding direction\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Response with LLM Before Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate LLM response\n",
    "def generates_response_before_fine_tuning(prompt, model):\n",
    "\n",
    "    # Apply the tokenizer\n",
    "    encoded_input = tokenizer(prompt,\n",
    "                              return_tensors = \"pt\",\n",
    "                              add_special_tokens = True)\n",
    "\n",
    "    # Transform the input into a tensor\n",
    "    model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "    # Generate the response\n",
    "    generated_ids = model.generate(**model_inputs,\n",
    "                                   max_new_tokens = 1024,\n",
    "                                   do_sample = True,\n",
    "                                   pad_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode the response\n",
    "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    return decoded_output[0].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Your goal is to determine the relationship between the two provided clinical sentences and classify them into one of the following categories:\n",
      "Contradiction: If the two sentences contradict each other. Neutral: If the two sentences are unrelated to each other. Entailment: If one of the sentences logically entails the other. Sentence 1: For his hypotension, autonomic testing confirmed orthostatic hypotension. Sentence 2: the patient has orthostatic hypotension <|end_of_turn|>AI Assistant:\n"
     ]
    }
   ],
   "source": [
    "# Prompt example\n",
    "prompt = \"\"\"Instruction: Your goal is to determine the relationship between the two provided clinical sentences and classify them into one of the following categories:\n",
    "Contradiction: If the two sentences contradict each other. Neutral: If the two sentences are unrelated to each other. Entailment: If one of the sentences logically entails the other. \"\"\"\n",
    "prompt += '''Sentence 1: For his hypotension, autonomic testing confirmed orthostatic hypotension. Sentence 2: the patient has orthostatic hypotension <|end_of_turn|>'''\n",
    "prompt += \"AI Assistant:\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Instruction: Your goal is to determine the relationship between the two provided clinical sentences and classify them into one of the following categories:\\nContradiction: If the two sentences contradict each other. Neutral: If the two sentences are unrelated to each other. Entailment: If one of the sentences logically entails the other. Sentence 1: For his hypotension, autonomic testing confirmed orthostatic hypotension. Sentence 2: the patient has orthostatic hypotension <|end_of_turn|> AI Assistant: Entailment<|end_of_turn|>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate response\n",
    "generates_response_before_fine_tuning(prompt, llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRa Parameters for PEFT\n",
    "\n",
    "## PEFT and LORA\n",
    "\n",
    "PEFT (Parameter-Efficient Fine-Tuning) and LoRA (Low-Rank Adaptation) are fine-tuning techniques used to train large language models more efficiently, reducing computational costs and the number of parameters required to adapt to new tasks.\n",
    "\n",
    "### 1. **PEFT (Parameter-Efficient Fine-Tuning)**:\n",
    "\n",
    "- **Description**: It is a technique that allows tuning large models with parameter efficiency. Instead of tuning all the model parameters during training, PEFT tunes only a small fraction of the parameters, which reduces training time and cost.\n",
    "- **Advantages**:\n",
    "- Requires fewer computational resources.\n",
    "- Improves memory efficiency.\n",
    "- Keeps most of the model frozen (unchanged), tuning only a specific part for new tasks.\n",
    "\n",
    "### 2. **LoRA (Low-Rank Adaptation)**:\n",
    "\n",
    "- **Description**: LoRA is a specific technique within the PEFT concept that fine-tunes models by training low-rank matrices instead of training all the model parameters. The idea is to insert small additional low-rank matrices in some of the model layers. Instead of training all the layers, only these extra matrices are trained, and the original weights remain frozen.\n",
    "- **How ​​it works**: LoRA decomposes the weight matrix of a layer into two smaller (or low-rank) matrices that, when multiplied, approximate the original weights. During fine-tuning, only these low-rank matrices are trained.\n",
    "- **Advantages**:\n",
    "- Efficient fine-tuning of large models without directly modifying the main weights.\n",
    "- Uses less memory and computing resources.\n",
    "- Facilitates fine-tuning for various tasks, keeping the model base unchanged.\n",
    "\n",
    "Both techniques are used to improve the efficiency of fine-tuning large models, such as GPT and BERT, for specific tasks, making them more accessible and feasible in resource-constrained environments.\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the description of each parameter in the LoraConfig configuration we'll use:\n",
    "\n",
    "**r**: Specifies the size of the expansion factor used in the LoRA (Low-Rank Adaptation) mechanism. A value of r = 8 indicates that low-rank adaptation increases the dimensionality of the projections by a factor of 8. Essentially, this parameter controls the size of the dimensionality increase that is applied to selected layers of the model to allow for greater adaptability without significantly changing the total number of model parameters.\n",
    "\n",
    "**lora_alpha**: Multiplier for the learning factor applied specifically to the LoRA adaptation parameters. With lora_alpha = 16, the learning of these specific parameters is increased by 16 times compared to the rest of the model parameters. This allows the adaptation parameters to adjust more quickly during training.\n",
    "\n",
    "**lora_dropout**: Dropout rate applied to LoRA modifications. A value of 0.05 means that 5% of the elements in the LoRA projections will be randomly zeroed out during training. Dropout is a regularization technique used to avoid overfitting by forcing the model to learn more robust representations since it cannot rely on any single connection between neurons.\n",
    "\n",
    "**bias**: Sets whether or not bias terms are used in LoRA fitting. Here, it is set to \"none\", indicating that no bias terms will be added to LoRA fitting. Choosing not to use bias can simplify the model and focus the fitting solely on the weights.\n",
    "\n",
    "**task_type**: Sets the type of task the model is being configured for. \"CAUSAL_LM\" refers to a causal language model, where the prediction of each subsequent word depends only on the previous words and not on any future words. This is typical of models that generate text in a sequential manner, such as text generation or dialogue modeling.\n",
    "\n",
    "These parameters are configured to tune the model effectively and specifically, leveraging the low-rank adaptation technique to fine-tune the model without the computational cost of re-training all the parameters of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines LoRa parameters\n",
    "peft_config = LoraConfig(r = 8,\n",
    "                             lora_alpha = 16,\n",
    "                             lora_dropout = 0.05,\n",
    "                             bias = \"none\",\n",
    "                             task_type = \"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for fine-tuning\n",
    "llm_model = prepare_model_for_kbit_training(llm_model)\n",
    "\n",
    "# Concatenate the base model with the LoRa parameters\n",
    "llm_model = get_peft_model(llm_model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the TrainingArguments object are used to configure the training process of a machine learning model. Here is the description of each of them:\n",
    "\n",
    "**output_dir**: Directory where the artifacts of the trained model, such as model weights and settings, will be saved. Here, it is set to \"fitted_model\".\n",
    "\n",
    "**per_device_train_batch_size**: Number of examples processed simultaneously on each device (such as a GPU) during training.\n",
    "\n",
    "**gradient_accumulation_steps**: Number of training steps to accumulate the gradients before performing a parameter update. This allows using a larger effective batch size than what can fit in the device's memory.\n",
    "\n",
    "**optim**: Optimizer used to update the model weights. \"paged_adamw_32bit\" is a variation of the AdamW optimizer that optimizes memory usage by using 32-bit precision.\n",
    "\n",
    "**learning_rate**: Initial learning rate used by the optimizer.\n",
    "\n",
    "**lr_scheduler_type**: Type of learning rate scheduler, which adjusts the learning rate based on the number of epochs or steps. \"cosine\" indicates that the learning rate follows a cosine curve, gradually decreasing over the epochs.\n",
    "\n",
    "**save_strategy**: Strategy for saving the model during training. \"epoch\" means that the model will be saved at the end of each epoch.\n",
    "\n",
    "**logging_steps**: Number of training steps after which logs will be recorded. Here, it is set to 10, meaning that the training progress will be logged every 10 steps.\n",
    "\n",
    "**num_train_epochs**: Total number of training epochs. An epoch is a complete pass through the training data.\n",
    "\n",
    "**max_steps**: Maximum number of training steps to be executed. Useful to terminate training after a fixed number of steps, regardless of epochs.\n",
    "\n",
    "**fp16**: If enabled, enables the use of 16-bit floating point during training to reduce memory usage and possibly speed up computation. Here, it is set to True, indicating that it is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the model training arguments\n",
    "training_arguments = TrainingArguments(output_dir = \"adjusted_model\",\n",
    "                                           per_device_train_batch_size = 1,\n",
    "                                           gradient_accumulation_steps = 4,\n",
    "                                           optim = \"paged_adamw_32bit\",\n",
    "                                           learning_rate = 2e-4,\n",
    "                                           lr_scheduler_type = \"cosine\",\n",
    "                                           save_strategy = \"epoch\",\n",
    "                                           logging_steps = 10,\n",
    "                                           num_train_epochs = 1,\n",
    "                                           max_steps = 250,\n",
    "                                           fp16 = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Fine-tuning Trainer (SFFT) Parameters\n",
    "\n",
    "The **Supervised Fine-Tuning Trainer** is a method used in the supervised training of large language models, such as GPT and BERT, to fine-tune their capabilities for a specific task. It involves updating the model's parameters based on labeled data, so that the model can learn to perform tasks such as classification, translation, or text generation more effectively.\n",
    "\n",
    "### Structure of the Supervised Fine-Tuning Trainer:\n",
    "\n",
    "1. **Pre-trained model**:\n",
    "- It starts with a model that has already been pre-trained on a large corpus of unsupervised data. This pre-training gives the model a solid foundation in natural language, but it is not yet fine-tuned for specific tasks.\n",
    "2. **Supervised data**:\n",
    "- For supervised fine-tuning, a labeled dataset is used, where the inputs have expected outputs. Example: For a sentiment classification task, texts would come with labels such as \"positive\" or \"negative\".\n",
    "3. **Objective**:\n",
    "- During supervised training, the goal is to minimize a loss function, which measures how different the model's output is from the expected output on the supervised data.\n",
    "4. **Parameter update**:\n",
    "- The model adjusts its parameters based on the predictions it makes regarding the data labels. The error is calculated and the **optimizer**, such as **Adam**, adjusts the weights of the model layers to reduce this error next time.\n",
    "\n",
    "### Workflow:\n",
    "1. **Forward pass**: The input data is passed through the model to generate a prediction.\n",
    "2. **Loss calculation**: The prediction is compared with the expected label and the loss function is calculated.\n",
    "3. **Backpropagation**: Through gradient descent, the errors are propagated to adjust the model parameters. 4. **Parameter update**: The optimizer updates the weights to reduce the error in the next iteration.\n",
    "\n",
    "### Tools:\n",
    "\n",
    "The supervised fine-tuning process is usually managed by frameworks such as **Hugging Face Transformers**, where the **Trainer** is a class that facilitates this process. It takes care of many technical aspects, such as:\n",
    "\n",
    "- Splitting the dataset into training and validation sets.\n",
    "- Calculating the loss function and updating the parameters.\n",
    "- Evaluating metrics of the model's performance.\n",
    "\n",
    "### Importance:\n",
    "\n",
    "The Supervised Fine-Tuning Trainer is crucial for customizing large models, such as GPT, for specific tasks, allowing the model to generalize well and perform better in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a description of each parameter used to create an SFTTrainer instance:\n",
    "\n",
    "**model**: Defines the machine learning model that will be trained.\n",
    "\n",
    "**peft_config**: Configuration for parameter efficient model training (PEFT).\n",
    "\n",
    "**max_seq_length**: Maximum sequence length that the model can accept. It is set to 512, which means that each input (such as text) will be truncated or padded to this length to maintain a consistent length across inputs.\n",
    "\n",
    "**tokenizer**: The tokenizer is used to convert text into a format that the model can process (such as tokens or numbers). Here, a specific tokenizer is used, which must be compatible with the chosen model.\n",
    "\n",
    "**packing**: A boolean parameter that, when true, indicates that the trainer should attempt to efficiently pack the input sequences to optimize memory usage and training performance.\n",
    "\n",
    "**formatting_func**: Function used to format the input data before it is fed to the model.\n",
    "\n",
    "**args**: Additional training configuration parameters, grouped in dsa_training_arguments. These arguments define various settings such as learning rate, saving strategy, among others, which were previously described.\n",
    "\n",
    "**train_dataset**: Dataset used to train the model.\n",
    "\n",
    "**eval_dataset**: Dataset used to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634092a36d8b4d869ff54477d4fff74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4a37e726d34badbf0b28b014cf3ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Define os parâmetros do SFTT\n",
    "trainer = SFTTrainer(model = llm_model,\n",
    "                         peft_config = peft_config,\n",
    "                         max_seq_length = 512,\n",
    "                         tokenizer = tokenizer,\n",
    "                         packing = True,\n",
    "                         formatting_func = create_prompt,\n",
    "                         args = training_arguments,\n",
    "                         train_dataset = dataset[\"train\"],\n",
    "                         eval_dataset = dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Training (Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e258230ba74f76859467b66f51f5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1541, 'grad_norm': 2.9172513484954834, 'learning_rate': 0.00019936113105200085, 'epoch': 0.02}\n",
      "{'loss': 1.8915, 'grad_norm': 1.6502469778060913, 'learning_rate': 0.00019745268727865774, 'epoch': 0.04}\n",
      "{'loss': 1.9087, 'grad_norm': 1.7545417547225952, 'learning_rate': 0.00019470983049947444, 'epoch': 0.06}\n",
      "{'loss': 1.6634, 'grad_norm': 1.9404270648956299, 'learning_rate': 0.0001899405251566371, 'epoch': 0.08}\n",
      "{'loss': 1.5109, 'grad_norm': 1.562394142150879, 'learning_rate': 0.0001837528040042142, 'epoch': 0.1}\n",
      "{'loss': 1.4148, 'grad_norm': 1.895841121673584, 'learning_rate': 0.0001762442511011448, 'epoch': 0.13}\n",
      "{'loss': 1.5128, 'grad_norm': 6.78122615814209, 'learning_rate': 0.00016753328081210245, 'epoch': 0.15}\n",
      "{'loss': 1.4127, 'grad_norm': 1.6238045692443848, 'learning_rate': 0.00015775727034222675, 'epoch': 0.17}\n",
      "{'loss': 1.3461, 'grad_norm': 1.735244870185852, 'learning_rate': 0.0001470703932165333, 'epoch': 0.19}\n",
      "{'loss': 1.4661, 'grad_norm': 1.481718897819519, 'learning_rate': 0.00013564118787132506, 'epoch': 0.21}\n",
      "{'loss': 1.4831, 'grad_norm': 2.0247726440429688, 'learning_rate': 0.00012364989970237248, 'epoch': 0.23}\n",
      "{'loss': 1.4945, 'grad_norm': 1.5082776546478271, 'learning_rate': 0.00011128563848734816, 'epoch': 0.25}\n",
      "{'loss': 1.4527, 'grad_norm': 1.6042344570159912, 'learning_rate': 9.874339601166473e-05, 'epoch': 0.27}\n",
      "{'loss': 1.457, 'grad_norm': 1.5473202466964722, 'learning_rate': 8.62209709315362e-05, 'epoch': 0.29}\n",
      "{'loss': 1.3694, 'grad_norm': 1.4184257984161377, 'learning_rate': 7.391584937101033e-05, 'epoch': 0.31}\n",
      "{'loss': 1.4726, 'grad_norm': 1.5009489059448242, 'learning_rate': 6.20220904478199e-05, 'epoch': 0.33}\n",
      "{'loss': 1.3262, 'grad_norm': 2.01230788230896, 'learning_rate': 5.072726584517086e-05, 'epoch': 0.35}\n",
      "{'loss': 1.2589, 'grad_norm': 1.603543758392334, 'learning_rate': 4.020950169424815e-05, 'epoch': 0.38}\n",
      "{'loss': 1.3146, 'grad_norm': 1.3158154487609863, 'learning_rate': 3.063466941871953e-05, 'epoch': 0.4}\n",
      "{'loss': 1.5206, 'grad_norm': 1.6307404041290283, 'learning_rate': 2.2153769843297667e-05, 'epoch': 0.42}\n",
      "{'loss': 1.2315, 'grad_norm': 1.2907284498214722, 'learning_rate': 1.4900551820530828e-05, 'epoch': 0.44}\n",
      "{'loss': 1.3378, 'grad_norm': 1.4483247995376587, 'learning_rate': 8.989402931500434e-06, 'epoch': 0.46}\n",
      "{'loss': 1.4109, 'grad_norm': 1.488808035850525, 'learning_rate': 4.513545525335705e-06, 'epoch': 0.48}\n",
      "{'loss': 1.3765, 'grad_norm': 1.1786855459213257, 'learning_rate': 1.543566547079467e-06, 'epoch': 0.5}\n",
      "{'loss': 1.3251, 'grad_norm': 1.2660640478134155, 'learning_rate': 1.2630433939825327e-07, 'epoch': 0.52}\n",
      "{'train_runtime': 1025.9084, 'train_samples_per_second': 0.975, 'train_steps_per_second': 0.244, 'train_loss': 1.4845002326965333, 'epoch': 0.52}\n",
      "CPU times: total: 1min\n",
      "Wall time: 17min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.4845002326965333, metrics={'train_runtime': 1025.9084, 'train_samples_per_second': 0.975, 'train_steps_per_second': 0.244, 'total_flos': 2.185444196352e+16, 'train_loss': 1.4845002326965333, 'epoch': 0.5211047420531527})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the adjusted model\n",
    "trainer.save_model(\"adjusted_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload the model and remove it from training mode\n",
    "final_model = llm_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Response Generation Function\n",
    "def generate_response_after_fine_tuning(prompt, model):\n",
    "\n",
    "    # Apply the tokenizer\n",
    "    encoded_input = tokenizer(prompt,\n",
    "                              return_tensors = \"pt\",\n",
    "                              add_special_tokens = True)\n",
    "\n",
    "    # Transforms the input into a tensor\n",
    "    model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "    # Generate the response\n",
    "    generated_ids = model.generate(**model_inputs,\n",
    "                                   max_new_tokens = 512,\n",
    "                                   do_sample = True,\n",
    "                                   use_cache = False,\n",
    "                                   pad_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode the response\n",
    "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    return decoded_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Instruction: In your role as a medical professional, address the user's medical questions and concerns. I have a white tab under my tounge that is not only painful when i touch it but bleeds as well. not sure what it is, or why I got it. Can you give me any advise? <|end_of_turn|>  AI Assistant: It sounds like you may have a lesion known as an ulcer. The most common cause of an oral ulcer is trauma, such as from a sharp tooth or the edges of dentures. If this is the case, stopping the trauma should help the ulcer to heal.\n",
      "\n",
      "If the ulcer is not due to trauma, it could be related to diet or may be viral or bacterial in origin. However, if the ulcer persists for longer than two to three weeks, it is important to check it out with your Dentist.<|end_of_turn|>\n",
      "CPU times: total: 3.05 s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Instruction: In your role as a medical professional, address the user's medical questions and concerns. \"\n",
    "prompt += \"I have a white tab under my tounge that is not only painful when i touch it but bleeds as well. not sure what it is, or why I got it. Can you give me any advise? <|end_of_turn|> \"\n",
    "prompt += \"AI Assistant:\"\n",
    "response = generate_response_after_fine_tuning(prompt, final_model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Instruction: In your capacity as a healthcare expert, offer insights and recommendations in response to users' medical inquiries. I have terrible anxiety and depression. I've tried various therapists and pills, but nothing's helped. <|end_of_turn|>  AI Assistant: Hi and Thank you for using the Healthcare Magician. In light of your history of an anxiety and depression, I understand your desire to find effective treatment for these conditions. When you say you have tried various therapists and pills, it's important to understand what specifically you have tried in terms of psychotropic medication. The best treatment for an anxiety or depression involves a combination of psychotherapy and psychotropic medications. It's important to note that not all medications are effective for all individuals. Thus, a 'trial and error' approach is often taken when looking for the 'right' treatment for you. In addition, cognitive behavioural therapy is the primary form of psychotherapy used for anxiety and depression but other therapies can be utilized depending on your condition. As for therapists, it's important that you find a good fit with regard to your individual personality and specific condition. That said, I still need more information on you (like duration of your anxiety/depression and your general medical history). It's also vital to understand if you are on any present medications or have under gone any treatments thus far due to your anxiety and depression. I hope you find this helpful.<|end_of_turn|>\n",
      "CPU times: total: 6.94 s\n",
      "Wall time: 51.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Instruction: In your capacity as a healthcare expert, offer insights and recommendations in response to users' medical inquiries. \"\n",
    "prompt += \"I have terrible anxiety and depression. I've tried various therapists and pills, but nothing's helped. <|end_of_turn|> \"\n",
    "prompt += \"AI Assistant:\"\n",
    "response = generate_response_after_fine_tuning(prompt, final_model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Instruction: As a medical chatbot, your responsibility is to provide information and guidance on medical matters to users. Hi sir, I am so happy with this website. First of all thanks for giving this opportunity. I am the  Software employee.My age is 24. My height is 169cm .Recently I got back pain and some pain in chest. How can i get relief from those pains.How i improve my health and which type of diseases will attack to my life in future. Please give Some health tips for heart and kidneys protection. <|end_of_turn|>  AI Assistant: Hello, I am glad to hear that you are interested in your health. Back pain and chest pain can be caused by various conditions. In order to identify the underlying cause, it is necessary to review your medical history and perform a thorough physical examination. Some of the common causes of back pain include muscle strain, disc degeneration, and arthritis. Chest pain can be caused by a variety of conditions including heart disease, apeksia nervosa, and anxiety disorders. It is important to rule out any serious medical conditions that could be causing your pain. To improve your overall health, it would be useful to start an exercise program that helps strengthen your back, core muscles, and overall cardiovascular fitness. Avoid smoking, alcohol, and excessive caffeine intake. Make sure to eat a balanced diet consisting of plenty of fruits and vegetables, whole grains, and lean proteins. Additionally, practicing stress management techniques such as deep breathing, yoga, or meditation can help reduce anxiety-related chest pain. Remember to consult with your doctor for further advice.<|end_of_turn|>\n",
      "CPU times: total: 4.92 s\n",
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Instruction: As a medical chatbot, your responsibility is to provide information and guidance on medical matters to users. \"\n",
    "prompt += \"Hi sir, I am so happy with this website. First of all thanks for giving this opportunity. I am the  Software employee.My age is 24. My height is 169cm .Recently I got back pain and some pain in chest. How can i get relief from those pains.How i improve my health and which type of diseases will attack to my life in future. Please give Some health tips for heart and kidneys protection. <|end_of_turn|> \"\n",
    "prompt += \"AI Assistant:\"\n",
    "response = generate_response_after_fine_tuning(prompt, final_model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
