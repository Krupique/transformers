{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers explanation\n",
    "\n",
    "### Paper oficial\n",
    "\n",
    "> [Attention is all you need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Transformers Architecture\n",
    "\n",
    "Transformers are in the spotlight, and for good reason. They have revolutionized the field over the past few years.  \n",
    "\n",
    "The Transformer is an architecture that leverages Attention to significantly enhance the performance of models designed for sequence learning tasks.  \n",
    "\n",
    "This architecture was first introduced in the 2017 paper *\"[Attention Is All You Need]((https://arxiv.org/abs/1706.03762))\"* and quickly established itself as the leading framework for most AI applications.  \n",
    "\n",
    "Since then, various projects, including Google's BERT and OpenAI's GPT series, have been built on this architecture, delivering incredible performance results that have easily surpassed existing state-of-the-art benchmarks.  \n",
    "\n",
    "The image below illustrates the standard Transformer architecture.  \n",
    "\n",
    "<img src=\"imagens/transformer_imagem01.png\" alt=\"IMG\" style=\"width: 600px;\"/>\n",
    "\n",
    "---\n",
    "\n",
    "## Encoder and Decoder Stacks  \n",
    "\n",
    "At its core, the Transformer architecture consists of a stack of encoder layers and decoder layers. To avoid confusion, we will refer to individual layers as *Encoder* or *Decoder* and use the terms *encoder stack* or *decoder stack* for a group of encoder or decoder layers.  \n",
    "\n",
    "Both the encoder stack and decoder stack include their respective embedding layers for their inputs. Finally, there is an output layer to generate the final output.  \n",
    "\n",
    "<img src=\"imagens/transformer_imagem02.jpeg\" alt=\"IMG\" style=\"width: 600;\"/>\n",
    "\n",
    "All encoders are identical to each other, as are all decoders.  \n",
    "\n",
    "The encoder includes the crucial self-attention layer, which computes relationships between different words in a sequence, as well as a feed-forward layer.  \n",
    "\n",
    "The decoder contains the self-attention layer, the feed-forward layer, and an additional encoder-decoder attention layer.  \n",
    "\n",
    "Each encoder and decoder has its own set of weights. There are many variations of the Transformer architecture. Some Transformer architectures do not include any decoders and rely solely on the encoder.  \n",
    "\n",
    "<img src=\"imagens/transformer_imagem03.jpeg\" alt=\"IMG\" style=\"width: 600;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Does Self-Attention Do?\n",
    "\n",
    "The key to the groundbreaking performance of the Transformer is its use of Attention, specifically Self-Attention.  \n",
    "\n",
    "When processing a word, Attention allows the model to focus on other words in the input that are closely related to that word.  \n",
    "\n",
    "For example, \"Ball\" is closely related to \"blue\" and \"hold.\" On the other hand, \"blue\" is not related to \"boy,\" as shown in the image below:  \n",
    "\n",
    "<img src=\"imagens/transformer_image04.png\" alt=\"IMG\" style=\"width: 370px;\" />  \n",
    "\n",
    "The Transformer architecture uses Self-Attention by relating each word in the input sequence to all other words.  \n",
    "\n",
    "---\n",
    "\n",
    "## The Transformer Model Training Process  \n",
    "\n",
    "The Transformer operates slightly differently during training and inference.  \n",
    "\n",
    "Let’s first look at the data flow during training. Training data consists of two parts:  \n",
    "\n",
    "- The source or input sequence (e.g., \"You are welcome\" in English, for a translation task).  \n",
    "- The target or output sequence (e.g., \"De nada\" in Portuguese).  \n",
    "\n",
    "The Transformer’s goal is to learn how to generate the target sequence using both the input sequence and the target sequence.  \n",
    "\n",
    "<img src=\"imagens/transformer_imagem05.jpeg\" alt=\"IMG\" style=\"width: 700px;\" />  \n",
    "\n",
    "The Transformer processes the data as follows:  \n",
    "\n",
    "1. The input sequence is converted into embeddings (with positional encoding) and fed into the encoder.  \n",
    "2. The encoder stack processes this and produces an encoded representation of the input sequence.  \n",
    "3. The target sequence, appended with a start-of-sentence token, is converted into embeddings (with positional encoding) and fed into the decoder.  \n",
    "4. The decoder stack processes this alongside the encoded representation from the encoder stack to produce an encoded representation of the target sequence.  \n",
    "5. The output layer converts the prior processing into word probabilities and the final output sequence.  \n",
    "6. The Transformer’s loss function compares this output sequence with the target sequence from the training data. This loss is used to generate gradients to train the Transformer during backpropagation.  \n",
    "\n",
    "---\n",
    "\n",
    "## The Transformer Model Inference Process  \n",
    "\n",
    "During inference, we only have the input sequence and do not have the target sequence to feed into the decoder. The Transformer’s objective is to generate the target sequence based solely on the input sequence.  \n",
    "\n",
    "Similar to a Seq2Seq model, the output is generated in a loop, with the output sequence from the previous time step fed into the decoder in the next time step until an end-of-sentence token is encountered.  \n",
    "\n",
    "The difference from the Seq2Seq model is that, at each time step, the entire generated output sequence so far is fed back into the decoder rather than just the last word.  \n",
    "\n",
    "<img src=\"imagens/transformer_imagem06.jpeg\" alt=\"IMG\" style=\"width: 700px;\" />  \n",
    "\n",
    "The data flow during inference is:  \n",
    "\n",
    "1. The input sequence is converted into embeddings (with positional encoding) and fed into the encoder.  \n",
    "2. The encoder stack processes this and produces an encoded representation of the input sequence.  \n",
    "3. Instead of the target sequence, an empty sequence with only a start-of-sentence token is used. This is converted into embeddings (with positional encoding) and fed into the decoder.  \n",
    "4. The decoder stack processes this alongside the encoded representation from the encoder stack to produce an encoded representation of the target sequence.  \n",
    "5. The output layer converts this into word probabilities and produces an output sequence.  \n",
    "6. The last word of the output sequence is taken as the predicted word. This word is then appended to the second position in the decoder input sequence, which now contains the start-of-sentence token and the first predicted word.  \n",
    "7. Return to step 3. As before, feed the new decoder sequence into the model. Then, take the second predicted word and append it to the decoder sequence. Repeat this until predicting an end-of-sentence token. Note that, since the encoder sequence does not change with each iteration, steps 1 and 2 do not need to be repeated every time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification and Language Models with Transformers\n",
    "\n",
    "Transformers are highly versatile and are used for most NLP tasks, such as language modeling and text classification. They are frequently employed in sequence-to-sequence models for applications like machine translation, text summarization, question answering, named entity recognition, and speech recognition.\n",
    "\n",
    "Different variants of the Transformer architecture are tailored to specific problems. The basic encoder layer serves as a common building block for these architectures, with different application-specific \"heads\" depending on the problem being addressed.\n",
    "\n",
    "---\n",
    "\n",
    "### Text Classification Architecture with Transformers  \n",
    "\n",
    "A sentiment analysis application, for instance, would take a text document as input. A classification head uses the Transformer’s output to generate predictions for class labels, such as positive or negative sentiment.  \n",
    "\n",
    "<img src=\"imagens/transformer_imagem07.jpeg\" alt=\"IMG\" style=\"width: 700px;\" />  \n",
    "\n",
    "---\n",
    "\n",
    "### Language Model Architecture with Transformers  \n",
    "\n",
    "A language model architecture takes the initial portion of an input sequence, such as a text phrase, and generates new text by predicting the subsequent phrases.  \n",
    "\n",
    "A language model head uses the Transformer’s output to generate probabilities for each word in the vocabulary. The word with the highest probability becomes the predicted next word in the sentence.  \n",
    "\n",
    "<img src=\"imagens/transformer_imagem08.jpeg\" alt=\"IMG\" style=\"width: 700px;\" />  \n",
    "\n",
    "---\n",
    "\n",
    "## How the Embedding Layer Works  \n",
    "\n",
    "Like any NLP model, the Transformer needs two pieces of information for each word: the meaning of the word and its position in the sequence.  \n",
    "\n",
    "- The Embedding layer encodes the meaning of the word.  \n",
    "- The Position Encoding layer represents the position of the word.  \n",
    "\n",
    "The Transformer combines these two encodings by adding them.  \n",
    "\n",
    "There are two embedding layers in the Transformer:  \n",
    "\n",
    "1. **Input Embedding Layer:** The input sequence is passed through this layer, which is also called input embedding.  \n",
    "   <img src=\"imagens/transformer_imagem11.jpeg\" alt=\"IMG\" style=\"width: 700px;\" />  \n",
    "\n",
    "2. **Output Embedding Layer:** The target sequence is fed into this layer after shifting the targets one position to the right and inserting a start token in the first position. During inference, since there is no target sequence, the output sequence is fed into this layer in a loop, which is why it’s called the Output Embedding Layer.  \n",
    "\n",
    "The text sequence is mapped to numerical word IDs using the vocabulary. The embedding layer maps each input word to an embedding vector, which is a richer representation of the word's meaning.  \n",
    "\n",
    "<img src=\"imagens/transformer_imagem12.jpeg\" alt=\"IMG\" style=\"width: 700px;\" />  \n",
    "\n",
    "---\n",
    "\n",
    "## How the Positional Encoding Layer Works  \n",
    "\n",
    "**Positional Encoding** is a critical technique in the Transformer architecture used to incorporate positional information into input sequences.  \n",
    "\n",
    "Since the Transformer architecture lacks recurrence or convolutions, it has no inherent knowledge of word order or relative positions within the sequence. Positional encoding resolves this issue by adding positional information to each word.  \n",
    "\n",
    "### How Positional Encoding Works  \n",
    "\n",
    "1. **Adding Positional Information:** For each word in the input sequence, a positional encoding vector is added to the word embedding vector. This positional encoding vector includes information about the word’s position in the sequence.  \n",
    "\n",
    "2. **Calculation Formula:** Positional encoding is computed using sine and cosine functions at different frequencies. The formula for each dimension of the positional encoding vector is:  \n",
    "\n",
    "**Even Dimensions (\\(i\\) is even):**\n",
    "\n",
    "$PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$\n",
    "\n",
    "**Odd Dimensions (\\(i\\) is odd):**\n",
    "\n",
    "$PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$\n",
    "\n",
    "### Where:\n",
    "- \\(pos\\): The position of the token in the sequence.\n",
    "- \\(2i\\): The even index of the dimension.\n",
    "- \\(2i+1\\): The odd index of the dimension.\n",
    "- \\(d_{model}\\): The dimensionality of the model embeddings.\n",
    "\n",
    "**Explanation:**\n",
    "- **Frequency scaling:** The denominator $(10000^{\\frac{2i}{d_{model}}})$ ensures different frequencies for each dimension.\n",
    "- **Sine and cosine:** Alternate dimensions use sine and cosine functions to encode positions uniquely.\n",
    "\n",
    "This encoding creates periodic patterns that help the model learn relationships between tokens based on their relative positions.\n",
    "\n",
    "3. **Why Use Sine and Cosine?**  \n",
    "   These functions allow the model to easily learn to represent relative distances between words. The combination of sine and cosine at varying frequencies ensures each position produces a unique vector while still allowing the model to generalize to unseen positions during training.  \n",
    "\n",
    "4. **Adding to Word Embeddings:**  \n",
    "   The positional encoding vector is added to the word embedding vector. This enables the model to process both content information (from the word embedding) and positional information for each word.  \n",
    "\n",
    "Positional encoding is essential for Transformer-based models because it provides the positional context needed to understand word order in a sequence. Without it, the Transformer would treat all inputs as unordered sets, which would be inadequate for many NLP tasks like translation, where word order is crucial for meaning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Dimensions\n",
    "\n",
    "Deep learning models process a batch of training samples at a time. The embedding and positional encoding layers operate on matrices that represent a batch of sequence samples. The embedding layer takes a matrix in the format (samples, sequence length) of word IDs. It encodes each word ID into a word vector of length equal to the embedding size, resulting in an output matrix in the format (samples, sequence length, embedding size).\n",
    "\n",
    "The positional encoding uses an encoding size equal to the embedding size, producing a similarly formatted matrix that can be added to the embedding matrix.\n",
    "\n",
    "<img src=\"imagens/transformer_imagem14.jpeg\" alt=\"IMG\" style=\"width: 600px;\" />\n",
    "\n",
    "The (samples, sequence length, embedding size) shape produced by the embedding and positional encoding layers is preserved throughout the Transformer as the data flows through the stacks of encoders and decoders until it is reshaped by the final output layers.\n",
    "\n",
    "This illustrates the 3D matrix dimensions in the Transformer. However, for simplicity, we'll omit the first dimension (for samples) in the following explanations and use 2D representations for a single sample.\n",
    "\n",
    "---\n",
    "\n",
    "## Encoder Stack Matrices\n",
    "\n",
    "The encoder and decoder stacks consist of several (typically six) encoders and decoders, respectively, connected sequentially.\n",
    "\n",
    "- The first encoder in the stack receives input from the embedding and positional encoding layers.  \n",
    "- Subsequent encoders receive input from the previous encoder in the stack.\n",
    "\n",
    "The encoder processes its input through a **multi-head self-attention layer.** The output of self-attention is passed to a **feed-forward layer,** which then sends its output to the next encoder.\n",
    "\n",
    "<img src=\"imagens/transformer_imagem15.png\" alt=\"IMG\" style=\"width: 370px;\" />\n",
    "\n",
    "Both the self-attention and feed-forward sublayers have a **residual connection** around them, followed by **layer normalization.**\n",
    "\n",
    "The output of the last encoder is passed to each decoder in the decoder stack, as explained below.\n",
    "\n",
    "---\n",
    "\n",
    "## Decoder Stack Matrices\n",
    "\n",
    "The decoder structure is similar to the encoder, with a few differences.\n",
    "\n",
    "- Like the encoder, the first decoder in the stack receives input from the output embedding and positional encoding layers.  \n",
    "- Subsequent decoders in the stack take input from the previous decoder.\n",
    "\n",
    "The decoder processes its input through a **multi-head self-attention layer,** which operates slightly differently than the encoder’s self-attention. It is restricted to attend only to **previous positions** in the sequence. This is achieved by **masking future positions,** a topic we will discuss later.\n",
    "\n",
    "<img src=\"imagens/transformer_imagem16.png\" alt=\"IMG\" style=\"width: 370px;\" />\n",
    "\n",
    "Unlike the encoder, the decoder includes a second **multi-head attention layer** called the **encoder-decoder attention layer.** This layer works similarly to self-attention but combines two input sources:  \n",
    "1. The self-attention layer below it.  \n",
    "2. The output from the encoder stack.\n",
    "\n",
    "The output from the encoder-decoder attention layer is passed to a **feed-forward layer,** which then sends its output to the next decoder.\n",
    "\n",
    "Each of these sublayers—self-attention, encoder-decoder attention, and feed-forward—has a **residual connection** around it, followed by **layer normalization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Self-Attention Mechanism Works\n",
    "\n",
    "Now, let's examine how various vectors/tensors flow through these components to transform a model's input into its output. As with most NLP applications, we begin by converting each input word into a vector using an embedding algorithm.\n",
    "\n",
    "Each word is embedded into a 512-dimensional vector (standard architecture). These vectors are represented by simple boxes in the image below:\n",
    "\n",
    "<img src=\"imagens/transformer_imagem09.png\" alt=\"IMG\" style=\"width: 750px;\" />\n",
    "\n",
    "The general abstraction for all encoders is that they take a list of 512-dimensional vectors as input.\n",
    "\n",
    "After embedding the words in our input sequence, each one flows through the two layers of the encoder.\n",
    "\n",
    "<img src=\"imagens/transformer_imagem10.png\" alt=\"IMG\" style=\"width: 750px;\" />\n",
    "\n",
    "Here, we observe a key property of the Transformer: each word at each position follows its unique path through the encoder. Dependencies exist between these paths in the **Self-Attention** layer. However, the **feed-forward layer** has no such dependencies, allowing parallel execution of these paths, which speeds up training.\n",
    "\n",
    "Next, we'll switch to a shorter sentence to explore what happens in each encoder sublayer.\n",
    "\n",
    "---\n",
    "\n",
    "## The Math of Self-Attention - Initial Vectors\n",
    "\n",
    "The first step in calculating Self-Attention is to create three vectors from each input vector of the encoder (i.e., the embedding of each word). For each word, we create a **query vector (Q)**, a **key vector (K)**, and a **value vector (V)**. These vectors are created by multiplying the embedding by three matrices trained during the training process.\n",
    "\n",
    "Note that these new vectors have dimensions smaller than the embedding vectors. Their dimensionality is 64, while the embedding and encoder input/output vectors are 512-dimensional. **This reduced dimensionality is an architectural choice to make multi-head attention computation more efficient.**\n",
    "\n",
    "Multiplying \\( x_1 \\) by the weight matrix \\( W_Q \\) produces \\( q_1 \\), the **query vector** associated with that word. Similarly, we create **queries, keys, and values** for each word in the input sentence. These vectors serve as useful abstractions for calculating attention.\n",
    "\n",
    "<img src=\"imagens/transformer_imagem17.png\" alt=\"IMG\" style=\"width: 750px;\" />\n",
    "\n",
    "---\n",
    "\n",
    "## The Math of Self-Attention - Score Calculation\n",
    "\n",
    "The second step is to calculate a **score.** Let’s say we are calculating Self-Attention for the first word in this example, “Thinking.”\n",
    "\n",
    "We need to score each word in the input sentence relative to this word. The score determines how much focus to place on other parts of the sentence when encoding a word in a specific position.\n",
    "\n",
    "The score is calculated by computing the **dot product** of the query vector with the key vector of the respective word. For example, if we’re processing Self-Attention for the word at position #1, the first score is the dot product of \\( q_1 \\) and \\( k_1 \\). The second score is the dot product of \\( q_1 \\) and \\( k_2 \\), and so on.\n",
    "\n",
    "<img src=\"imagens/transformer_imagem18.png\" alt=\"IMG\" style=\"width: 750px;\" />\n",
    "\n",
    "---\n",
    "\n",
    "## The Math of Self-Attention - Softmax Operation\n",
    "\n",
    "The third and fourth steps involve dividing the scores by 8 (the square root of the key vector dimension, 64, from the original paper). This scaling ensures more stable gradients. Other values could be used here, but this is the standard.\n",
    "\n",
    "The results are then passed through a **softmax** operation. Softmax normalizes the scores so they are all positive and sum to 1.\n",
    "\n",
    "<img src=\"imagens/transformer_imagem19.png\" alt=\"IMG\" style=\"width: 750px;\" />\n",
    "\n",
    "The **softmax scores** determine how much each word contributes to the attention for a specific position. The word at the current position typically has the highest softmax score, but other relevant words may also contribute significantly.\n",
    "\n",
    "---\n",
    "\n",
    "## The Math of Self-Attention - Weighted Sum\n",
    "\n",
    "The fifth step is to multiply each **value vector** by its softmax score. This effectively amplifies the values of relevant words while dampening the values of irrelevant ones.\n",
    "\n",
    "The sixth step is to **sum the weighted value vectors.** This produces the output of the Self-Attention layer for the specific position (in this case, for the first word).\n",
    "\n",
    "<img src=\"imagens/transformer_imagem20.png\" alt=\"IMG\" style=\"width: 750px;\" />\n",
    "\n",
    "This completes the Self-Attention calculation. The resulting vector can then be passed to the feed-forward neural network. However, in programming implementations, this calculation is performed in matrix form for faster processing.\n",
    "\n",
    "---\n",
    "\n",
    "## The Math of Self-Attention - Scaled Dot Product and Final Formulation\n",
    "\n",
    "In general, a Self-Attention mechanism requires four components:\n",
    "\n",
    "- **Queries (Q):** Represent the positions being encoded.\n",
    "- **Keys (K):** Compared against the queries to determine attention levels.\n",
    "- **Values (V):** Weighted by attention scores and summed to produce the layer's output.\n",
    "- **Scoring Function:** Determines which elements require more attention. It takes a query and key as input and outputs an attention weight.\n",
    "\n",
    "The most common scoring function implementation is the **dot product** (or a small MLP comparing similarity metrics).\n",
    "\n",
    "The **scaled dot product attention** mechanism calculates how much one part of the input should focus on another using the dot product of input vectors. This mechanism is used in both encoder and decoder layers.\n",
    "\n",
    "- First, the dot product between queries and keys is calculated.\n",
    "- Next, the result is scaled by dividing by the square root of the key vector dimension, which prevents excessively large gradients during training in deep networks.\n",
    "- After scaling, a **softmax function** is applied to produce **attention weights** (probabilities) indicating the relative importance of each value.\n",
    "\n",
    "The final attention output is obtained by weighting the values by these probabilities and summing them.\n",
    "\n",
    "The mathematical formulation for **scaled dot product attention** is:\n",
    "\n",
    "$\\text{Scaled Dot-Product Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "Where\n",
    "\n",
    "- **$(QK^T)$** computes the similarity between the query and all keys.\n",
    "- **$(\\frac{1}{\\sqrt{d_k}})$** is used to scale the dot product and avoid excessively large values, which could result in numerical instability.\n",
    "- The **$softmax$** function converts the attention scores into a probability distribution.\n",
    "- The final output is a weighted sum of the values \\(V\\), based on the computed attention weights.\n",
    "\n",
    "And there you have it: **Attention is all you need!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Expression:\n",
    "\n",
    "\n",
    "### Key Points:\n",
    "- **\\( QK^T \\)** computes the similarity between the query and all keys.\n",
    "- **\\( \\frac{1}{\\sqrt{d_k}} \\)** is used to scale the dot product and avoid excessively large values, which could result in numerical instability.\n",
    "- The **softmax** function converts the attention scores into a probability distribution.\n",
    "- The final output is a weighted sum of the values \\(V\\), based on the computed attention weights.\n",
    "\n",
    "This mechanism allows the model to focus on different parts of the input sequence based on the relevance of each token to the current query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functioning of the Feed-Forward Layer and Mathematical Formulation\n",
    "\n",
    "### Formula\n",
    "\n",
    "$FFN(x) = ReLU(W_1x + b_1)W_2 + b_2$\n",
    "\n",
    "<img src=\"imagens/transformer_imagem03.jpeg\" alt=\"IMG\" style=\"width: 370px;\" />\n",
    "\n",
    "The formula above represents a Feed-Forward Neural Network (FFN) with one hidden layer and the ReLU activation function. Here's a step-by-step breakdown:\n",
    "\n",
    "**FFN(x):** Represents the feed-forward neural network function, which takes an input 'x' and produces an output. This output results from processing the input through the network layers.\n",
    "\n",
    "**ReLU:** The Rectified Linear Unit activation function. It operates simply: if the input is positive, it returns the input; if negative, it returns zero. This introduces non-linearity into the model, essential for learning complex patterns.\n",
    "\n",
    "**W₁ and W₂:** These are the weights of the network's two layers. They are matrices that transform the input data into the outputs for each layer. The weights are adjusted during training to minimize prediction error.\n",
    "\n",
    "**x:** The input vector for the network.\n",
    "\n",
    "**b₁ and b₂:** Bias terms for the two layers. These biases are added after the weight multiplications and before applying the activation function. Like weights, they are adjusted during training and make the neural network more flexible.\n",
    "\n",
    "**W₁x + b₁:** The linear combination of the input vector 'x' and the weights and bias of the first layer.\n",
    "\n",
    "**ReLU(W₁x + b₁):** The ReLU activation function is applied to the first linear combination, setting negative values to zero while leaving positive values unchanged.\n",
    "\n",
    "**(ReLU(W₁x + b₁))W₂ + b₂:** The activated output from the first layer is multiplied by the weights of the second layer, and the second bias is added, producing the final network output.\n",
    "\n",
    "This structure is common in dense neural networks, where each neuron in one layer is connected to every neuron in the next. These networks can be trained for tasks like classification or regression, depending on the number of output units and the loss function used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Layer Normalization Works and Mathematical Formulation\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$\\text{layernorm}(x + \\text{sublayer}(x))$\n",
    "\n",
    "<img src=\"imagens/transformer_imagem31.png\" alt=\"IMG\" style=\"width: 700px;\" />\n",
    "\n",
    "<img src=\"imagens/transformer_imagem29.png\" alt=\"IMG\" style=\"width: 470px;\" />\n",
    "\n",
    "The formula above represents a typical operation within a Transformer block, specifically within one of its many attention or feed-forward neural network sublayers. Here’s a breakdown of what this operation does:\n",
    "\n",
    "### Components:\n",
    "\n",
    "- **layernorm**: Refers to **layer normalization**, a normalization technique applied to activation vectors within the network. Normalizing data at each layer stabilizes learning and allows the model to train faster and more effectively.\n",
    "\n",
    "- **x**: Represents the input vector for the current sublayer of the neural network. In the context of a Transformer, \\( x \\) could be the output of a previous sublayer, such as an attention layer.\n",
    "\n",
    "- **sublayer(x)**: Represents a function that performs a transformation on the input vector \\( x \\). Depending on the context, **sublayer** can be an attention layer, a feed-forward layer, or any other operation. For example, in a Transformer, this might involve self-attention or positional feed-forward computations.\n",
    "\n",
    "- **x + sublayer(x)**: Represents a **residual connection** or **skip connection**, where the output of the sublayer is added to the original input vector. Residual connections are crucial for effectively training deep networks, as they help mitigate issues like vanishing or exploding gradients, commonly observed in other deep learning architectures such as RNNs and LSTMs.\n",
    "\n",
    "- **layernorm(x + sublayer(x))**: After adding the original input vector to the sublayer output, layer normalization is applied. The resulting normalized vector is then passed to the next sublayer or layer.\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "This formula describes the process of passing an input \\( x \\) through a sublayer (e.g., attention or feed-forward network), adding the sublayer output to the original input via residual connections, and applying layer normalization to the result. This structure is a critical part of the Transformer architecture, allowing the model to \"remember\" the original input while incorporating newly processed information.\n",
    "\n",
    "Normalization benefits the model by reducing training time, making it less biased toward higher-value features, and preventing gradient explosions or vanishing by constraining gradients to a specific range. In summary, training a model with unnormalized features in gradient descent is suboptimal, making normalization essential. The key decision is choosing the type of normalization.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Normalization:\n",
    "\n",
    "<img src=\"imagens/transformer_imagem30.png\" alt=\"IMG\" style=\"width: 470px;\" />\n",
    "\n",
    "1. **Batch Normalization**:\n",
    "   - Involves all sentences in a batch.\n",
    "   - For each feature across sentences, calculates a mean and variance for normalization.\n",
    "   - Ensures the mean is close to zero and variance close to one for all features.\n",
    "   - This process is repeated for every feature in the input data.\n",
    "\n",
    "2. **Layer Normalization**:\n",
    "   - Calculates the mean and variance of all features within a single sentence.\n",
    "   - It doesn’t depend on batch size or whether sentences are in the same batch.\n",
    "   - Simply uses all features of a single sentence for normalization.\n",
    "\n",
    "Layer normalization was initially designed for recurrent neural networks (RNNs) because batch normalization's performance depends on mini-batch size. The Transformer architecture uses layer normalization throughout the model as it works exceptionally well for NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Representation of Multi-Head Attention\n",
    "\n",
    "The Transformer processes multiple attention heads in parallel, referring to each as an \"Attention Head.\" This is known as **Multi-Head Attention**.\n",
    "\n",
    "<img src=\"imagens/transformer_imagem22.png\" alt=\"IMG\" style=\"width: 500px;\" />\n",
    "\n",
    "### Process:\n",
    "\n",
    "1. **Queries (Q), Keys (K), and Values (V)**:\n",
    "   - These are generated by passing the input through separate linear layers, each with its own weights.\n",
    "   - This produces three outputs: \\( Q \\), \\( K \\), and \\( V \\), which are then combined using the attention formula below to produce the **Attention Scores**.\n",
    "\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
    "\n",
    "<img src=\"imagens/transformer_imagem21.png\" alt=\"IMG\" style=\"width: 350px;\" /><br/>\n",
    "<img src=\"imagens/transformer_imagem23.png\" alt=\"IMG\" style=\"width: 500px;\" />\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- The \\( Q \\), \\( K \\), and \\( V \\) values contain encoded representations of each word in the sequence.\n",
    "- The **self-attention mechanism** computes attention scores, combining each word with all others in the sequence.\n",
    "- These scores encode how much each word should focus on every other word in the sequence.\n",
    "\n",
    "### Masking in Attention:\n",
    "\n",
    "While discussing the decoder earlier, masking was briefly mentioned. The mask (illustrated in the attention diagrams) controls what the model focuses on during training. For example, in causal settings, it prevents attention to future words. This ensures predictions rely only on prior context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Masks\n",
    "\n",
    "When calculating the attention scores, the Attention module implements a masking step. This serves two purposes:\n",
    "\n",
    "1. **In Encoder Self-Attention and Encoder-Decoder Attention**:  \n",
    "   Masking zeros out attention outputs where padding tokens exist in the input sentences. This ensures that padding does not contribute to self-attention.  \n",
    "   _(Note: Since input sequences may have different lengths, they are padded with tokens, as in most NLP models, to enable fixed-length vectors to be passed into the Transformer.)_\n",
    "\n",
    "2. **In Decoder Self-Attention**:  \n",
    "   Masking prevents the decoder from \"seeing\" the rest of the target sentence while predicting the next word.  \n",
    "\n",
    "The decoder processes words from the source sequence and uses them to predict words in the target sequence. During training, **Teacher Forcing** is used, where the complete target sequence is fed as input to the decoder. As a result, while predicting a word at a given position, the decoder has access to preceding and succeeding target words, allowing it to \"cheat\" by using future target words.\n",
    "\n",
    "For example, when predicting **Word 3**, the decoder should only refer to the first three words of the target input and not the fourth word (**AI**).\n",
    "\n",
    "<img src=\"imagens/transformer_imagem24.png\" alt=\"IMG\" style=\"width: 500px;\" />\n",
    "\n",
    "To address this, the Decoder masks the input words that appear later in the sequence.  \n",
    "\n",
    "---\n",
    "\n",
    "### How Masking Works:\n",
    "\n",
    "When calculating attention scores, masking is applied to the numerator just before the Softmax. Masked elements are set to negative infinity, so that Softmax converts those values to zero.\n",
    "\n",
    "<img src=\"imagens/transformer_imagem25.png\" alt=\"IMG\" style=\"width: 500px;\" />\n",
    "\n",
    "---\n",
    "\n",
    "## Output Generation Process\n",
    "\n",
    "The final decoder in the stack passes its output to the Output Component, which converts it into the final output sentence.\n",
    "\n",
    "- The **Linear Layer** projects the decoder vector into **Word Scores**, with one score for every unique word in the target vocabulary, at each position in the sentence.  \n",
    "  - For example, if the final sentence has 7 words and the target vocabulary has 10,000 unique words, 10,000 score values are generated for each of the 7 words.  \n",
    "  - These scores represent the likelihood of each vocabulary word appearing at that position in the sentence.  \n",
    "\n",
    "This helps explain why a large language model (LLM) can have billions of parameters.\n",
    "\n",
    "- The **Softmax Layer** transforms these scores into probabilities (values between 0 and 1).  \n",
    "  - At each position, the word index with the highest probability is selected and mapped to the corresponding word in the vocabulary.  \n",
    "  - These selected words form the Transformer's output sequence.\n",
    "\n",
    "<img src=\"imagens/transformer_imagem26.png\" alt=\"IMG\" style=\"width: 500px;\" />\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "During training, a **loss function**, such as cross-entropy loss, is used to compare the generated output probability distribution with the target sequence. The probability distribution represents the likelihood of each word occurring at each position.\n",
    "\n",
    "<img src=\"imagens/transformer_imagem27.png\" alt=\"IMG\" style=\"width: 800px;\" />\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "\n",
    "Assume the target vocabulary contains only four words. The goal is to produce a probability distribution that matches the expected target sequence: **\"De nada END\"**.\n",
    "\n",
    "- For the first word position, the probability distribution should assign a probability of 1 to \"De\" and 0 to all other words.  \n",
    "- Similarly, \"welcome\" and \"END\" should have probabilities of 1 in the second and third positions, respectively.\n",
    "\n",
    "The loss function computes the difference between the predicted and expected distributions. This loss is then used to calculate gradients to train the Transformer via backpropagation. The training process here is the same as in other deep learning architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
